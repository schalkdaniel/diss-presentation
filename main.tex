\documentclass[t,10pt]{beamer}

\usepackage{natbib}
\bibliographystyle{apalike}

%% Packages:
%% --------------------------------------------------------------

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{bbm}
\usepackage{natbib}
% \usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algpseudocodex}
\usepackage{float}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usetikzlibrary{shapes,arrows,automata,positioning,calc}
\usepackage{subfig}
% \usepackage{paralist}
\usepackage{graphicx}
\usepackage{array}
\usepackage{framed}
\usepackage{excludeonly}
\usepackage{fancyvrb}
\usecolortheme{dove}
% \usefonttheme{serif}
\usepackage{xfrac}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{caption}
\captionsetup[figure]{labelformat=empty}
\usepackage{transparent}
\usepackage{blkarray}
\usepackage{bibentry}
\usepackage{mathdots}
\usepackage{graphbox}
\usepackage{xspace}

%%% KABLE HEADER:
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{tikz}
%\usetikzlibrary{calc}
\usetikzlibrary{fit,calc}

\definecolor{olivedrab}{RGB}{154,205,50}
\newcommand*{\tikzmk}[1]{\tikz[remember picture,overlay,] \node (#1) {};\ignorespaces}
%\newcommand{\tikzmark}[1]{\tikz[remember picture] \node[coordinate] (#1) {#1};}

\newcommand{\boxtrans}{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=white,opacity=.7,fit={($(A)+(-0.5,0.35\baselineskip)$)($(B)+(0.5\linewidth,-1.03\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxtranstwo}{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=white,opacity=.7,fit={($(A)+(-0.5,0.1\baselineskip)$)($(B)+(0.5\linewidth,-1.03\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxtransthree}{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=white,opacity=.7,fit={($(A)+(-2.5,0.23\baselineskip)$)($(B)+(\linewidth,-1.03\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxtransalgo}{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=white,opacity=.7,fit={($(A)+(-2.5,1.5\baselineskip)$)($(B)+(\linewidth,-1.03\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxtransfour}{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=white,opacity=.7,fit={($(A)+(-2.5,0.34\baselineskip)$)($(B)+(\linewidth,-1.03\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxit}[1]{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(-0.5,0.1\baselineskip)$)($(B)+(0.35\linewidth,-0.5\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxittwo}[1]{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(-0.35,0.3\baselineskip)$)($(B)+(0.15\linewidth,-0.5\baselineskip)$)}] {};}\ignorespaces}
\newcommand{\boxitthree}[1]{\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(0.05,0.4\baselineskip)$)($(B)+(0.05\linewidth,-0.4\baselineskip)$)}] {};}\ignorespaces}


%\renewcommand\topstrut[1][1.2ex]{\setlength\bigstrutjot{#1}{\bigstrut[t]}}
%\renewcommand\botstrut[1][0.9ex]{\setlength\bigstrutjot{#1}{\bigstrut[b]}}
\newcommand{\todo}{{\color{red}\textbf{TODO:}}\hspace{0.1cm}}
\newcommand{\penMat}{\bm{K}}
\newcommand{\idMat}{\bm{I}}

\newcommand{\fSlide}[2]{
\begin{frame}[plain]{}%
  \vspace{4cm}%
  \Large #1\\[0.2cm]%
  {\LARGE\textbf{#2}}%
	\addtocounter{framenumber}{-1}%
\end{frame}%
}


\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}


%% For speaker notes:
%% -------------------------------------------------------------

%\usepackage{pgfpages}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}

%%!! Run with `pdfpc --notes=right main.pdf

%% Custom Commands:
%% --------------------------------------------------------------

\usepackage{scalerel,stackengine}
\stackMath
\newcommand\reallywidehat[1]{%
\savestack{\tmpbox}{\stretchto{%
  \scaleto{%
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern.1pt\mathchar"0362\kern.1pt}%
    {\rule{0ex}{\textheight}}%WIDTH-LIMITED CIRCUMFLEX
  }{\textheight}%
}{2.4ex}}%
\stackon[-6.9pt]{#1}{\tmpbox}%
}
\parskip 1ex

%\newcommand*{\tran}{{\mkern-1.5mu\mathsf{T}}}
%\newcommand{\AUC}{\text{AUC}}
%\newcommand{\eAUC}{\reallywidehat{\AUC}}
%\def\oplogit{\mathop{\sf logit}}
%\newcommand{\logit}[1]{\oplogit\left(#1\right)}
%\renewcommand{\ln}{\mathop{\sf ln}}
%\def\var{\mathop{\sf var}}
%\def\mean{\mathop{\sf m}}
%\def\ci{\mathop{\sf ci}}
%\def\evar{\reallywidehat{\var}}
%\newcommand{\ROC}{\text{ROC}}

% \definecolor{metropolis_theme_color}{RGB}{35,55,59}
\definecolor{metropolis_theme_color}{RGB}{42,42,42}

%% Color customizations:
\definecolor{blue}{RGB}{0,155,164}
\definecolor{lime}{RGB}{175,202,11}
\definecolor{green}{RGB}{0,137,62}
\definecolor{titleblue}{RGB}{4,58,63}
\definecolor{deepskyblue}{RGB}{0,191,255}
\definecolor{mygrey}{RGB}{240,240,240}
\definecolor{chighlight}{RGB}{139,35,35}

\setbeamercolor{frametitle}{fg=mygrey, bg=metropolis_theme_color}
\setbeamercolor{progress bar}{fg=metropolis_theme_color}
\setbeamercolor{background canvas}{bg=white}

\setbeamertemplate{frame numbering}{%
  \insertframenumber{}/\inserttotalframenumber
}
\makeatother

\setbeamertemplate{footline}[text line]{%
    \noindent\hspace*{\dimexpr-\oddsidemargin-1in\relax}%
     \colorbox{metropolis_theme_color}{
     \makebox[\dimexpr\paperwidth-2\fboxsep\relax]{
     \color{mygrey}
     \begin{minipage}{0.33\linewidth}
       \secname
     \end{minipage}\hfill
     \begin{minipage}{0.33\linewidth}
       \centering
       \insertshortauthor
     \end{minipage}\hfill
     \begin{minipage}{0.33\linewidth}
       \flushright
       \insertframenumber{}/\inserttotalframenumber
     \end{minipage}
     }}%
  \hspace*{-\paperwidth}
}





%% Shaded for nicer code highlighting:
%% ---------------------------------------------------------------

% Define Shaded if not defined:
\makeatletter
\@ifundefined{Shaded}{%
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}%
}{}
\makeatother

\renewenvironment{Shaded}{
  \begin{mdframed}[
    backgroundcolor=mygrey,
    linecolor=metropolis_theme_color,
    rightline=false,
		leftline=false
  ]}{
  \end{mdframed}
}

%% Input custom stuff:

\input{commands}

%% Titlepage:
%% --------------------------------------------------------------

\title{Modern approaches for component-wise boosting:}
\subtitle{Automation, efficiency, and distributed computing with application to the medical domain}
\date{March 24, 2023}
\author{\textbf{Daniel Schalk}}
\institute{\textbf{Supervisor:} Prof. Dr. Bernd Bischl\\
\textbf{Reviewers:} Prof. Dr. Matthias Schmid, PD Dr. Fabian Scheipl\\
\textbf{Chair of the examination panel:} Prof. Dr. Christian Heumann}
\titlegraphic{
    \vspace{3cm}\hspace{5.4cm}\transparent{0.1}\includegraphics[height=8.5cm]{figures/LMU.png}
    \vspace{-3cm}
}

%% Text:
%% --------------------------------------------------------------

\begin{document}

\maketitle
\nobibliography*
\newcommand{\newblockold}{\newblock}
\newcommand{\newblocknew}{\hspace{0.1cm}\tiny}

\section*{About the dissertation}

\begin{frame}{Overview}
  \textbf{Focus of the dissertation:}
  \begin{itemize}
    \item[] Discuss and propose modern directions for component-wise gradient boosting \citep[CWB;][]{buhlmann2003boosting}.
  \end{itemize}
  \textbf{Contribution and topics:}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{figures/topics.png}
  \end{figure}
\end{frame}

\begin{frame}{Publications}
    \begin{minipage}[t]{0.8\textwidth}
        Part I - Efficiency (CWB)
        \tiny
        \renewcommand{\newblock}{\newblocknew}
        \begin{itemize}
            \item[{[}1{]}] {\tiny\bibentry{schalk2018compboost}}
            \item[{[}2{]}] {\tiny\bibentry{schalk2022accelerated}}
        \end{itemize}
        \normalsize
        \vspace{0.5cm}
        Part II - Automation
        \tiny
        \begin{itemize}
            \item[{[}3{]}] {\tiny\bibentry{coors2021autocompboost}}
        \end{itemize}
        \normalsize
        \vspace{0.8cm}
        Part III - Distributed computing 
        \tiny
        \begin{itemize}
            \item[{[}4{]}] {\tiny\bibentry{schalk2022distcwb}. [Currently under review in the journal \textit{Statistics and Computing}]}
            \item[{[}5{]}] {\tiny\bibentry{schalk2022dauc} [Currently under review in the journal \textit{BMC Medical Research Methodology}]}
            \item[{[}6{]}] {\tiny\bibentry{schalk2022dsBinVal}}
        \end{itemize}
        \renewcommand{\newblock}{\newblockold}
    \end{minipage}
    \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-2.5)$) {\includegraphics[width=0.25\textwidth]{figures/topics-cwb.png}};
    \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-0.35)$) {\includegraphics[width=0.25\textwidth]{figures/topics-autocwb.png}};
    \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,+1.8)$) {\includegraphics[width=0.25\textwidth]{figures/topics-dcwb.png}};
    \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,+3.5)$) {\includegraphics[width=0.25\textwidth]{figures/topics-dauc.png}};
\end{frame}

%\begin{frame}{Part I - Efficiency}
%  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-2.5)$) {\includegraphics[width=0.3\textwidth]{figures/topics-cwb.png}};
%  \begin{minipage}[t]{0.8\linewidth}
%    \textbf{Goal:} Increase CWB's efficiency
%    \begin{itemize}
%      \item \textbf{Acceleration:} Speed up the fitting process by using Nesterovs momentum.
%      \item \textbf{Memory:} Reduce the memory consumption by discretizing numerical features.
%    \end{itemize}
%  \end{minipage}
%  \vspace{0.3cm}
%
%  \textbf{Publications:}
%  \renewcommand{\newblock}{\newblocknew}
%  \begin{itemize}
%    \item[{[}1{]}] {\footnotesize\bibentry{schalk2018compboost}}
%    \item[{[}2{]}] {\footnotesize\bibentry{schalk2022accelerated}}
%  \end{itemize}
%  \renewcommand{\newblock}{\newblockold}
%\end{frame}
%
%\begin{frame}{Part II - Interpretable AutoML Framework}
%  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-2.5)$) {\includegraphics[width=0.3\textwidth]{figures/topics-autocwb.png}};
%  \begin{minipage}[t]{0.8\linewidth}
%    \textbf{Goal:}
%    \begin{itemize}
%      \item
%        Easy access to an interpretable AutoML framework based on CWB as fitting engine.
%      \item
%        Focus is the assessment of the required complexity to model a given task.
%    \end{itemize}
%  \end{minipage}
%  \vspace{0.3cm}
%
%  \textbf{Publication:}
%  \renewcommand{\newblock}{\newblocknew}
%  \begin{itemize}
%    \item[{[}3{]}] {\footnotesize\bibentry{coors2021autocompboost}}
%  \end{itemize}
%  \renewcommand{\newblock}{\newblockold}
%\end{frame}
%
%\begin{frame}{Part III - Distributed Computing}
%  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-2.5)$) {\includegraphics[width=0.3\textwidth]{figures/topics-dcwb.png}};
%  \begin{minipage}[t]{0.8\linewidth}
%    \textbf{Goal:}
%    \begin{itemize}
%      \item
%        Distributed and privacy-preserving computation of CWB.
%      \item
%        Estimation of common shared effects and site-specific effect corrections.
%    \end{itemize}
%  \end{minipage}
%  \vspace{0.3cm}
%
%  \textbf{Publications:}
%  \renewcommand{\newblock}{\newblocknew}
%  \begin{itemize}
%    \item[{[}4{]}] {\footnotesize\bibentry{schalk2022distcwb}. [Currently under review in the journal \textit{Statistics and Computing}]}
%  \end{itemize}
%  \renewcommand{\newblock}{\newblockold}
%
%\end{frame}
%
%\begin{frame}{Part III - Distributed Computing}
%  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(-5,-2.5)$) {\includegraphics[width=0.3\textwidth]{figures/topics-dauc.png}};
%  \begin{minipage}[t]{0.8\linewidth}
%  \textbf{Goal:}
%    \begin{itemize}
%      \item
%        Methodology and implementation of a distributed and privacy-preserving ROC analysis.
%    \end{itemize}
%  \end{minipage}
%  \vspace{0.3cm}
%
%  \textbf{Publications:}
%  \renewcommand{\newblock}{\newblocknew}
%  \begin{itemize}
%    \item[{[}5{]}] {\footnotesize\bibentry{schalk2022dauc} [Currently under review in the journal \textit{BMC Medical Research Methodology}]}
%    \item[{[}6{]}] {\footnotesize\bibentry{schalk2022dsBinVal}}
%  \end{itemize}
%  \renewcommand{\newblock}{\newblockold}
%
%\end{frame}

%\begin{frame}{Overview}
%  \begin{figure}
%    \centering
%    \includegraphics[width=0.6\textwidth]{figures/topics.png}
%  \end{figure}\vspace{-0.2cm}
%  The focus of this presentation is on CWB adjustments {[}1{]}, {[}2{]}, and {[}4{]}. The other contributions are briefly summarized at the end of the presentation.
%\end{frame}

\begin{frame}{Overview}
  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/topics-relevant.png}
  \end{figure}\vspace{-0.2cm}
  The focus of this presentation is on CWB adjustments {[}1{]}, {[}2{]}, and {[}4{]}. The other contributions are briefly summarized at the end of the presentation.
    \addtocounter{framenumber}{-1}
\end{frame}


\begin{frame}[plain]{Structure of the talk}
    \tableofcontents
    \addtocounter{framenumber}{-1}
\end{frame}



\section{Background}

\begin{frame}{Data}
  Example throughout this presentation is a subset of a WHO data set\footnote[frame,1]{Available at \url{kaggle.com/datasets/kumarajarshi/life-expectancy-who}} about life expectation in years per country:

  \scriptsize
  \input{tex/tab-example}
  \normalsize
  \begin{itemize}
      \item  
        $n = 75$ (for \texttt{Country} $\in$ $\{$\texttt{GER}, \texttt{USA}, \texttt{SWE}, \texttt{ZAF}, \texttt{ETH}$\}$, $n = 2938$ for all countries) 
      \item $p = 20$
  \end{itemize}
\end{frame}

\fSlide{Background}{Component-wise gradient boosting}

\begin{frame}{GB algorithm}
  \begin{algorithm}[H]
  \footnotesize
  \caption{Gradient boosting (GB) algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \State $\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$
          \State $\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame}{GB algorithm}
  \addtocounter{algorithm}{-1}
  \begin{algorithm}[H]
  \tikzmk{A}
  \footnotesize
  \caption{Gradient boosting (GB) algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}\tikzmk{B}\boxtransalgo
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \State \tikzmk{A}$\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$
          \State $\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$\tikzmk{B}\boxtransthree
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  The pseudo residuals $\rmm$ contain the information in which direction to move $\fmdh$ to better fit the data.
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{GB algorithm}
  \addtocounter{algorithm}{-1}
  \begin{algorithm}[H]
  \tikzmk{A}
  \footnotesize
  \caption{Gradient boosting (GB) algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$\tikzmk{B}\boxtransalgo
          \State $\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$
          \State \tikzmk{A}$\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$\tikzmk{B}\boxtransthree
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  One base learner $b$ (often a tree) is fitted to $\rmm$.
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{GB algorithm}
  \addtocounter{algorithm}{-1}
  \begin{algorithm}[H]
  \tikzmk{A}
  \footnotesize
  \caption{Gradient boosting (GB) algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \State $\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$
          \State $\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$\tikzmk{B}\boxtransalgo
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \tikzmk{A}\textbf{return} $\fh = \fh^{[M]}$\tikzmk{B}\boxtransthree
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  The fitted base learner is added to the ensemble by conducting a functional gradient descent step. 
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{CWB basics}
  \begin{itemize}
    \item
      Compared to GB, CWB can choose from a set of $K$ base learners $b \in \{b_1, \dots, b_K\}$.

    \item
      Often, $b_1, \dots, b_K$ are chosen to be (interpretable) statistical models and hence $f$ corresponds to a generalized additive model~\citep[GAM;][]{hastie2017generalized}: \[f(\xv) = f_0 + \sum_{k=1}^K b_k(\xv | \tb_k), \ \ \text{intercept}\ f_0\]

    \item
      Advantages of CWB:
      \begin{itemize}
        \item
          Feasible to get fit in high-dimensional feature spaces ($p \gg n$).

        \item
          An inherent (unbiased) feature selection.

        \item
          Interpretable/explainable partial feature effects (depending on the choice of base learners).
      \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}{GB algorithm}
\addtocounter{algorithm}{-1}
  \begin{algorithm}[H]
  \tikzmk{A}
  \footnotesize
  \caption{GB algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$\tikzmk{B}\boxtransalgo
          \State \tikzmk{A}$\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$\tikzmk{B}\boxittwo{red}
          \State \tikzmk{A}$\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$\tikzmk{B}\boxtransthree
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame}{CWB algorithm}
  \begin{algorithm}[H]
  \tikzmk{A}
  \footnotesize
  \caption{Vanilla CWB algorithm}\label{algo:cwb}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{CWB}$}{$\D,\nu,M,L,b_1, \dots, b_\blK$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$\tikzmk{B}\boxtransalgo
          \For{\tikzmk{A}$\blk \in \{1, \dots, \blK\}$}
              \State $\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$
              \State $\sse_\blk = \sum_{i=1}^n(\rmi - b_\blk(\xi | \tbmh_\blk))^2$
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK\}} \sse_\blk$
          \tikzmk{B}
          \boxit{olivedrab}
          \State \tikzmk{A}$\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_{\blk^{[m]}})$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$\tikzmk{B}\boxtransfour
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{CWB algorithm}
  \addtocounter{algorithm}{-1}
  \begin{algorithm}[H]
  \footnotesize
  \caption{Vanilla CWB algorithm}\label{algo:cwb}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{CWB}$}{$\D,\nu,M,L,b_1, \dots, b_\blK$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \For{$\blk \in \{1, \dots, \blK\}$}
              \State $\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$
              \State $\sse_\blk = \sum_{i=1}^n(\rmi - b_\blk(\xi | \tbmh_\blk))^2$
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK\}} \sse_\blk$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_i{\blk^{[m]}})$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Base learner}
  \begin{itemize}
    \item
      Each base learner $b_k$ has a basis transformation $g_k : \Xspace \to \R^{d_k}$ with \[g_k(\xv) = (g_{k,1}(\xv), \dots, g_{k,d_k}(\xv))^\tran.\] and is linear in the parameters: $b_k(\xv | \tb_k) = g_k(\xv)^\tran \tb_k$

    \item
      For $n$ data points $\xi[1], \dots, \xi[n]$, each base learner defines a design matrix: \[
      \design_k = \left(\begin{array}{c}
      g_{k}^\tran(\xi[1]) \\
      \vdots \\
      g_{k}^\tran(\xi[n])\end{array}\right)\in\R^{n\times d_k}\]

    \item
      Each base learner can have a penalty matrix $\penMat_k$ (e.g. $\penMat_k = \lambda_k\bm{I}_{d_k}$ for ridge regression) and is fitted using the least squares estimator \[\tbh_k = (\design_\blk^\tran \design_\blk + \penMat_k)^{-1} \design_k^\tran \yv.\]
  \end{itemize}
\end{frame}


\begin{frame}{B/P-Spline base learner}
  \vspace{-0.3cm}\[g_k(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran\] B-spline basis $B$ of a pre-defined degree~\citep{eilers1996flexible}.
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-base/fig-bs0.png}
    \end{figure}
    \vspace{-0.3cm}
    \[
    \design_k = \left(\begin{array}{c}
      g_{k}^\tran(\xi[1]) \\
      \vdots \\
      g_{k}^\tran(\xi[n])
    \end{array}\right) = \left(\begin{array}{ccc}
      B_{k,1}(\xi[1]) & \dots & B_{k,d_k}(\xi[1]) \\
      \vdots &  & \vdots \\
      B_{k,1}(\xi[n]) & \dots & B_{k,d_k}(\xi[n])
    \end{array}\right)\in\R^{n\times d_k}
    \]
  \end{center}
\end{frame}

\input{tex/tex-bmat-anim.tex}

\begin{frame}{B/P-spline base learner}
  \vspace{-0.3cm}\[g_k(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran\] B-spline basis $B$ of a pre-defined degree~\citep{eilers1996flexible}.
\begin{center}
    \begin{tabular}{ccccc}
         $\design_k$ & $\Rightarrow$ & $\tbh_k = (\design_k^\tran \design_k + \penMat_k)^{-1}\design_k^\tran y$ & $\Rightarrow$ & \includegraphics[align=c,width=5cm]{figures/bs-estimated.pdf}
    \end{tabular}
\end{center}

\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat0.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
    \design_k = \left(\begin{array}{c}
      g_{k}^\tran(\xi[1]) \\
      \vdots \\
      g_{k}^\tran(\xi[n])
    \end{array}\right) = \left(\begin{array}{ccc}
      \mathds{1}_{\{\xi[1] = 1\}} & \dots & \mathds{1}_{\{\xi[1] = G\}} \\
      \vdots &  & \vdots \\
      \mathds{1}_{\{\xi[n] = 1\}} & \dots & \mathds{1}_{\{\xi[n] = G\}} \\
    \end{array}\right)\in\R^{n\times G}
    \]
  \end{center}

\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat1.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color[HTML]{631879}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \color{black}\\
        \phantom{x}
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat2.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{3B4992}\bm{1} \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \color{black}\\
        \phantom{x}\\
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat3.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} \\
        \color[HTML]{EE0000}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \color{black}\\
        \phantom{x}\\
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat4.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} \\
        \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
        \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008B45}\bm{1} & \color{lightgray}0 & \color{lightgray}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \color{black}\\
        \phantom{x}\\
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat5.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} \\
        \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
        \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008280}\bm{1} & \color{lightgray}0 \\
        \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \color{black}\\
        \phantom{x}\\
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
  \begin{center}
    \begin{figure}
      \includegraphics[width=0.7\textwidth]{figures/bs-cat/fig-cat6.png}
    \end{figure}
    \vspace{-0.5cm}
    \[
      \design_k = \tiny\begin{blockarray}{ccccc}
        \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
      \begin{block}{(ccccc)}
        \phantom{x}\\
        \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} \\
        \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
        \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{1} & \color{lightgray}0 \\
        \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008B45}\bm{1} & \color{lightgray}0 & \color{lightgray}0 \\
        \phantom{x}\\
      \end{block}
    \end{blockarray}
    \]
    \normalsize
  \end{center}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Categorical base learner}
  \vspace{-0.3cm}\[g_k(x) = (g_{k,1}(x), \dots, g_{k,G}(x))^\tran = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran, \ \ x\in\{1, \dots, G\}\]
\begin{center}
    \begin{tabular}{ccccc}
         $\design_k$ & $\Rightarrow$ & $\tbh_k = (\design_k^\tran \design_k + \penMat_k)^{-1}\design_k^\tran y$ & $\Rightarrow$ & \includegraphics[align=c,width=5cm]{figures/bcat-estimated.pdf}
    \end{tabular}
\end{center}
\end{frame}





\begin{frame}{(Row-wise) tensor product (RWTP) base learner}
  Combination (interaction) $b_k \odot b_l$ between to base learners $b_k$ and $b_l$:
  \[g_k(\xv) \otimes g_l(\xv) = (g_{k,1}(\xv)g_l(\xv)^\tran, \dots, g_{k,d_k}(\xv)g_l(\xv)^\tran)^\tran \]
  Design matrix:
  \[
  \design_k \odot \design_l = \left(\begin{array}{c}
    (g_{k}(\xi[1]) \otimes g_l(\xi[1]))^\tran \\
    \vdots \\
    (g_{k}(\xi[n]) \otimes g_l(\xi[n]))^\tran
  \end{array}\right) \in\R^{n\times d_k d_l}
  \]
\end{frame}





\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color[HTML]{631879}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(2)})} \\
      \color{white}\bm{g_l(x^{(3)})} & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
      \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(73)})} & \color{white}0 & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(74)})} & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(75)})} & \color{white}0 & \color{white}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
\end{frame}

\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color{black}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{3B4992}\bm{g_l(x^{(2)})} \\
      \color{white}\bm{g_l(x^{(3)})} & \color{white}0 & \color{white}0 & \color{white}0 & \color{white}0 \\
      \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(73)})} & \color{white}0 & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(74)})} & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(75)})} & \color{white}0 & \color{white}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color{black}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(2)})} \\
      \color[HTML]{EE0000}\bm{g_l(x^{(3)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots & \color{white}\vdots \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(73)})} & \color{white}0 & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(74)})} & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(75)})} & \color{white}0 & \color{white}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color{black}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(2)})} \\
      \color{black}\bm{g_l(x^{(3)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
      \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008B45}\bm{g_l(x^{(73)})} & \color{lightgray}0 & \color{lightgray}0 \\
      \color{white}0 & \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(74)})} & \color{white}0 \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(75)})} & \color{white}0 & \color{white}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color{black}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(2)})} \\
      \color{black}\bm{g_l(x^{(3)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
      \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(73)})} & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008280}\bm{g_l(x^{(74)})} & \color{lightgray}0 \\
      \color{white}0 & \color{white}0 & \color{white}\bm{g_l(x^{(75)})} & \color{white}0 & \color{white}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{RWTP base learner}
  Example:
  \begin{itemize}
    \item $b_k$ encodes the country: $g_k(x) = (\mathds{1}_{\{x = 1\}}, \dots, \mathds{1}_{\{x = G\}})^\tran$
    \item $b_l$ uses a B-spline basis for BMI: $g_l(x) = (B_{k,1}(x), \dots, B_{k,d_k}(x))^\tran$
  \end{itemize}
  $$
    \design_k \odot \design_l = \tiny\begin{blockarray}{ccccc}
      \color[HTML]{EE0000}g_{k,GER} & \color[HTML]{631879}g_{k,USA} & \color[HTML]{008B45}g_{k,SWE} & \color[HTML]{008280}g_{k,ZAF} & \color[HTML]{3B4992}g_{k,ETH}\\
    \begin{block}{(ccccc)}
      \phantom{x}\\
      \color{lightgray}0 & \color{black}\bm{g_l(x^{(1)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(2)})} \\
      \color{black}\bm{g_l(x^{(3)})} & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots & \color{lightgray}\vdots \\
      \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(73)})} & \color{lightgray}0 & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color{lightgray}0 & \color{black}\bm{g_l(x^{(74)})} & \color{lightgray}0 \\
      \color{lightgray}0 & \color{lightgray}0 & \color[HTML]{008B45}\bm{g_l(x^{(75)})} & \color{lightgray}0 & \color{lightgray}0 \\
      \phantom{x}\\
    \end{block}
  \end{blockarray}
  $$
  \normalsize
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-GER.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-USA.png}}
  {\transparent{1}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-SWE.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ZAF.png}}
  {\transparent{0.2}\includegraphics[width=0.19\textwidth]{figures/fig-bs0-ETH.png}}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{RWTP base learner}
  \begin{itemize}
    \item Categoric - numeric base learner combination:
      \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/bs-tensor/fig-cat-num.png}
      \end{figure}
    \item Numeric - numeric base learner combination:
      \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/bs-tensor/fig-num-num.png}
      \end{figure}
  \end{itemize}
\end{frame}


%\fSlide{Component-wise gradient boosting}{Fitting process}

%\input{tex/fig-cwb-anim.tex}

%\begin{frame}{Component-wise gradient boosting -- Example}
%	\begin{figure}
%		\centering
%		\includegraphics[width=\textwidth]{figures/cwb-anim/fig-iter-0150.png}
%	\end{figure}
%	\addtocounter{framenumber}{-1}
%\end{frame}


\section{Efficiency}

%\begin{frame}{Efficiency problems of CWB}
%
%  Computational complexity in terms of memory and runtime efficiency.
%  \begin{itemize}
%    \item \textbf{W.r.t. runtime:}
%    \begin{itemize}
%      \item Training CWB requires to (theoretically) calculate $(\design_k^\tran \design_k + \bm{K}_\blk)^{-1}\design_k^\tran \rmm$ for all $k = 1, \dots, K$ and $M$ iterations.
%      \item The computational load is tremendously reduced by pre-calculating the Cholesky decompositions of $\design_k^\tran \design_k + \bm{K}_\blk$ for all $k$ as iteration independent part and to re-cycle these matrices in each iteration.
%      \item Nevertheless, for big $K$ and $M$ this remains very costly and time consuming.
%    \end{itemize}
%    \item[] $\Rightarrow$ Fitting the algorithm can take too much time.
%  \end{itemize}
%\end{frame}
%
%\begin{frame}{Efficiency problems of CWB}
%
%  Computational complexity in terms of memory and runtime efficiency.
%  \begin{itemize}
%    \item \textbf{W.r.t. memory:}
%    \begin{itemize}
%      \item Each base learner requires to store a design matrix.
%      \item For example, using B-splines, it is common to to store an $n\times 24$ matrix (for $20$ knots, and cubic basis functions) for one numerical feature.
%    \end{itemize}
%    \item[] $\Rightarrow$ The RAM is filled very fast.
%  \end{itemize}
%\end{frame}

\begin{frame}{Efficiency problems of CWB}
    \textbf{W.r.t. runtime:}
    \begin{itemize}
        \item 
            The base learner selection in each iteration is an extensive task.% due to fitting each $b_k$ to $\rmm$. 
        \item
            Gradient descent converges rather slowly compared to the optimizer like momentum or Nesterov's momentum.\\[0.2cm]
            $\rightarrow$ Fitting the algorithm can be time consuming.
    \end{itemize}
    \textbf{W.r.t. memory:}  
    \begin{itemize}
    \item 
        Each base learner is required to store a design matrix.
    \item 
        Additional information in each iteration about pseudo residuals, predicted scores, parameters, etc. may be stored.\\[0.2cm]
        $\rightarrow$ The RAM is filled very fast.
  \end{itemize}
  \begin{itemize}
    \item[$\Rightarrow$] Less attractive or infeasible to use CWB for medium- to large-scale applications.
  \end{itemize}
\end{frame}

\begin{frame}{Publication~\citep{schalk2022accelerated}}
%  \begin{minipage}{0.7\textwidth}
%        {\tiny
%        \renewcommand{\newblock}{\newblocknew}
%        \begin{itemize}
%            \item[{[}2{]}] {\tiny\bibentry{schalk2022accelerated}}
%        \end{itemize}}
%        \renewcommand{\newblock}{\newblockold}
%  \end{minipage}
%  \begin{minipage}{0.25\textwidth}
%  %\vspace{-0.2cm}
%  \begin{figure}
%    \centering
%    \frame{\includegraphics[width=0.95\linewidth]{figures/fig-cacb-paper.png}}
%  \end{figure}
%  \end{minipage}
  
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.5\textwidth]{figures/fig-cacb-paper.png}}
  \end{figure}
  \textbf{Contributions:}
  \begin{itemize}
    \item
      \textbf{Accelerate the fitting process} of CWB by incorporating Nesterov's momentum.
    \item
      \textbf{Reduce the memory load} by implementing a more efficient data representation for numerical features.
  \end{itemize}
\end{frame}

\fSlide{Efficiency}{Accelerating component-wise boosting}

%\begin{frame}{Idea}
%
%  \textbf{Gradient descent:}
%
%  \vspace{0.2cm}
%  {\small
%  \begin{tabular}{ccc}
%    Parameter space & & Function space \\[0.3cm]
%    $\tbh^{[m+1]} = \tbh^{[m]} + \nu \nabla_{\tb}\riske(\fh(. | \tbh^{[m]}) | \D)$ & $\Rightarrow$ & $\fh^{[m+1]} = \fmh + \nu \hat{b}^{[m]}$
%  \end{tabular}}
%  \vspace{0.4cm}
%
%  \textbf{Nesterov's momentum:}
%
%  \vspace{0.2cm}
%  {\small
%  \begin{tabular}{ccc}
%    Parameter space & & Function space \\[0.3cm]
%    $\bm{u}^{[m]} = \nabla_{\tb}\riske(\fh(. | \tbh^{[m]} - \gamma \hat{\bm{\vartheta}}^{[m-1]}) | \D)$ &  & \\
%    $\hat{\bm{\vartheta}}^{[m]} = \gamma \hat{\bm{\vartheta}}^{[m-1]} + \nu \bm{u}^{[m]}$ & $\Rightarrow$ & ??? \\
%    $\tbh^{[m+1]} = \tbh^{[m]} - \hat{\bm{\vartheta}}^{[m]}$ & &
%  \end{tabular}}
%  \vspace{0.2cm}
%
%  \begin{itemize}
%  \item[$\Rightarrow$] \textbf{Idea:} Use Nesterov's momentum and adjust it for functional updates and CWB.
%  \end{itemize}
%
%\end{frame}

\begin{frame}{Accelerated gradient boosting machine}
  \begin{itemize}
    \item
      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
      \begin{alignat*}{2}
      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}             &&\\
      f^{[m+1]} &= g^{[m]} + \nu b^{[m]}                               &&\text{primary model}\\
      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m b^{[m]}_{\text{cor}} &&\text{momentum model}
      \end{alignat*}
  \end{itemize}
\end{frame}

\begin{frame}{Accelerated gradient boosting machine}
  \begin{itemize}
    \item
      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
      \begin{alignat*}{2}
      {\color{gray}g^{[m]}} &{\color{gray}\hspace{0.08cm}= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}}             &&\\
      f^{[m+1]} &= g^{[m]} + \nu b^{[m]}                               &&\text{primary model}\\
      {\color{gray}h^{[m+1]}} &{\color{gray}\hspace{0.08cm}= h^{[m]} + \gamma\nu / \theta_m b^{[m]}_{\text{cor}}} &&{\color{gray}\text{momentum model}}
      \end{alignat*}
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Accelerated gradient boosting machine}
  \begin{itemize}
    \item
      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
      \begin{alignat*}{2}
      {\color{gray}g^{[m]}} &{\color{gray}\hspace{0.08cm}= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}}             &&\\
      f^{[m+1]} &= g^{[m]} + \nu b^{[m]}                               &&\text{primary model}\\
      {\color{gray}h^{[m+1]}} &{\color{gray}\hspace{0.08cm}= h^{[m]} + \gamma\nu / \theta_m} {\color{blue}b^{[m]}_{\text{cor}}} &&{\color{gray}\text{momentum model}}
      \end{alignat*}
      \item A {\color{blue}second base learner $b^{[m]}_{\text{cor}}$} is fitted to the accumulated previous errors (\enquote{error-corrected pseudo residuals}) to accelerate the fitting into that direction.
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Accelerated gradient boosting machine}
  \begin{itemize}
    \item
      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
      \begin{alignat*}{2}
      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]} &&\\
      f^{[m+1]} &= g^{[m]} + \nu {\color{red}b^{[m]}} &&\text{primary model}\\
      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m {\color{blue}b^{[m]}_{\text{cor}}}&&\text{momentum model}
      \end{alignat*}
%      \[\Rightarrow\ f^{[m+1]} = f^{[m]} + \nu b^{[m]} + \theta_m (h^{[m]} - f^{[m]})\]%\vspace{0.1cm}
%    \item Momentum in the direction of $h^{[m]} - f^{[m]}$.
    \item A {\color{blue}second base learner $b^{[m]}_{\text{cor}}$} is fitted to the accumulated previous errors (\enquote{error-corrected pseudo residuals}) to accelerate the fitting into that direction.
    \item In each iteration, two base learners ${\color{red}b^{[m]}}$ and ${\color{blue}b^{[m]}_{\text{cor}}}$ are fitted.
    %\item ($\theta_m = 2 / (m+2),\ m = 0, \dots, M - 1$, momentum $\gamma \in (0,1]$)
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

%\begin{frame}{Accelerated gradient boosting machine}
%  \begin{itemize}
%    \item
%      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
%      \begin{align*}
%      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}\\
%      f^{[m+1]} &= g^{[m]} + \nu b^{[m]} \\
%      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m b^{[m]}_{\text{cor}}
%      \end{align*}
%      \[\Rightarrow\ f^{[m+1]} = f^{[m]} + \nu b^{[m]} + \theta_m (h^{[m]} - f^{[m]})\]%\vspace{0.1cm}
%  \end{itemize}
%  \addtocounter{framenumber}{-1}
%\end{frame}
%
%\begin{frame}{Accelerated gradient boosting machine}
%  \begin{itemize}
%    \item
%      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
%      \begin{align*}
%      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}\\
%      f^{[m+1]} &= g^{[m]} + \nu b^{[m]} \\
%      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m b^{[m]}_{\text{cor}}
%      \end{align*}
%      \[\Rightarrow\ f^{[m+1]} = f^{[m]} + \nu b^{[m]} + {\color{red}\theta_m (h^{[m]} - f^{[m]})}\]%\vspace{0.1cm}
%    \item Momentum in the direction of ${\color{red}h^{[m]} - f^{[m]}}$.
%  \end{itemize}
%  \addtocounter{framenumber}{-1}
%\end{frame}
%
%\begin{frame}{Accelerated gradient boosting machine}
%  \begin{itemize}
%    \item
%      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
%      \begin{align*}
%      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}\\
%      f^{[m+1]} &= g^{[m]} + \nu {\color{red}b^{[m]}} \\
%      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m {\color{blue}b^{[m]}_{\text{cor}}}
%      \end{align*}
%      \[\Rightarrow\ f^{[m+1]} = f^{[m]} + \nu b^{[m]} + \theta_m (h^{[m]} - f^{[m]})\]%\vspace{0.1cm}
%    \item Momentum in the direction of $h^{[m]} - f^{[m]}$.
%    \item In each iteration, two base learners ${\color{red}b^{[m]}}$ and ${\color{blue}b^{[m]}_{\text{cor}}}$.
%    \item ($\theta_m = 2 / (m+2),\ m = 0, \dots, M - 1$, momentum $\gamma \in (0,1]$)
%  \end{itemize}
%  \addtocounter{framenumber}{-1}
%\end{frame}

\begin{frame}{Accelerated component-wise boosting}
  In \citet{schalk2022accelerated}, we incorporated the updating scheme of AGBM into an accelerated CWB (ACWB)
  version:
  \begin{itemize}
    \item
      Both base learners \(b^{[m]}\in\{b_1, \dots, b_K\}\) and \(b^{[m]}_{\text{cor}}\in\{b_1, \dots, b_K\}\) that are added in one iteration are the
      result of a selection process w.r.t. to the minimal SSE.
    \item
      Update the parameter estimations accordingly to allow the estimation of partial feature effects.
  \end{itemize}
  Considering these points allows maintaining all advantages of CWB in ACWB.
\end{frame}

\begin{frame}{ACWB Algorithm}
   \begin{figure}
       \centering
       \includegraphics[width=0.85\textwidth]{figures/fig-acwb.png}
   \end{figure} 
\end{frame}

\begin{frame}{Hybrid component-wise boosting}
  \begin{itemize}
    \item
      ACWB can overfit or overshoot an optimal solution if not stopped early.
    \item
      Therefore, a hybrid CWB (HCWB) approach combines ACWB for an accelerated fitting in the beginning and CWB
      to fine-tune the model:
  \end{itemize}

  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/fig-HCWB.pdf}
    \caption{\small(Figure reference: \citet{schalk2022accelerated})}
  \end{figure}
\end{frame}

\fSlide{Efficiency}{Reduced memory consumption for numeric base learner}

\begin{frame}{Binning}
  \begin{itemize}
    \item
      To reduce the memory consumption, we applied binning to operate on a
      reduced representation of \(\bm{Z}_k\).
    \item
      Binning is a technique that allows to represent the \(n\) values
      \(x_k^{(1)}, \dots, x_k^{(n)}\) of \(\xv_k\) by \(n^\ast < n\) design
      points \(\bm{z}_k = (z_k^{(1)}, \dots, z_k^{(n^\ast)})\).
    \item
      The idea is to assign each \(x_k^{(i)}\) to the closest design point
      \(z_k^{(i)}\) and store the assignment in a map
      \(\text{ind}_k^{(i)}\):
      \(x_k^{(i)} \approx z_k^{(\text{ind}_k^{(i)})}\)
  \end{itemize}

  \begin{center}\includegraphics[width=9cm]{figures/binning/fig-xbin.png} \end{center}
\end{frame}

\begin{frame}{Binning}
  \begin{itemize}
    \item
      \citet{lang2014multilevel} used binning to discretize feature vectors
      to increase the efficiency of multilevel structured additive
      regression.
    \item
      \citet{wood2017gigadata} applied binning to fit GAMs to gigadata and
      argue that the best approximation is achieved by setting
      \(n^\ast = \sqrt{n}\).
    \item
      \citet{li2020faster} presented optimized cross-product operations of
      binned design matrices to also speed up the fitting.

    \item
        CWB is especially suited for binning since each base learner can apply binning individually and benefits from faster matrix operations.
  \end{itemize}
\end{frame}

\begin{frame}{Initializing a base learner with binning}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 234 \\ 73 \\ 498 \\ \vdots \\ 112 \\ 261 \\ 343 \end{pmatrix}}_{= \bm{x}_{k}\in\R^n}$}
    {\normalsize $\Rightarrow$}
    {\tiny $\underbrace{
      \begin{pmatrix}
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 & \color{black}0.62 & \color{black}0.31 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{black}0.20 & \color{black}0.66 & \color{black}0.14 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17\color{black}\\
        \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots &
          \color{black}\vdots & \color{black}\vdots & \color{black}\vdots\\
        \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 & \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.01 & \color{black}0.40 & \color{black}0.55 & \color{black}0.04 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.29 & \color{black}0.63 &
          \color{black}0.08 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
      \end{pmatrix}
    }_{=\bm{Z}_{k}\in\R^{n\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.1cm}
  {\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.5cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
\end{frame}

\begin{frame}{Initializing a base learner with binning}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 234 \\ 73 \\ 498 \\ \vdots \\ 112 \\ 261 \\ 343 \end{pmatrix}}_{= \bm{x}_{k}\in\R^n}$}
    {\transparent{0.2}\normalsize $\Rightarrow$}
    {\transparent{0.2}\tiny $\underbrace{
      \begin{pmatrix}
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 & \color{black}0.62 & \color{black}0.31 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{black}0.20 & \color{black}0.66 & \color{black}0.14 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17\color{black}\\
        \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots &
          \color{black}\vdots & \color{black}\vdots & \color{black}\vdots\\
        \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 & \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.01 & \color{black}0.40 & \color{black}0.55 & \color{black}0.04 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.29 & \color{black}0.63 &
          \color{black}0.08 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
      \end{pmatrix}
    }_{=\bm{Z}_{k}\in\R^{n\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.1cm}
  {\transparent{0.2}\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.5cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\transparent{0.2}\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
  \phantom{a.}\includegraphics[width=9cm]{figures/binx-iter1.png}
  \addtocounter{framenumber}{-1}
\end{frame}


\begin{frame}{Initializing a base learner with binning}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 234 \\ 73 \\ 498 \\ \vdots \\ 112 \\ 261 \\ 343 \end{pmatrix}}_{= \bm{x}_{k}\in\R^n}$}
    {\transparent{0.2}\normalsize $\Rightarrow$}
    {\transparent{0.2}\tiny $\underbrace{
      \begin{pmatrix}
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 & \color{black}0.62 & \color{black}0.31 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{black}0.20 & \color{black}0.66 & \color{black}0.14 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17\color{black}\\
        \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots &
          \color{black}\vdots & \color{black}\vdots & \color{black}\vdots\\
        \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 & \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.01 & \color{black}0.40 & \color{black}0.55 & \color{black}0.04 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.29 & \color{black}0.63 &
          \color{black}0.08 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
      \end{pmatrix}
    }_{=\bm{Z}_{k}\in\R^{n\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.1cm}
  {\transparent{0.2}\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.5cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\transparent{0.2}\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
  \phantom{a.}\includegraphics[width=9cm]{figures/binx-iter2.png}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 7.0 \\ 129.8 \\ 252.5 \\ 375.2 \\ 498.0 \end{pmatrix}}_{= \bm{z}_{k}\in\R^{n^\ast}}$}
  {\transparent{0}\normalsize $\Rightarrow$}
  {\transparent{0}\tiny $\underbrace{
    \begin{pmatrix}
      \color{black}0.17 & \color{black}0.67 & \color{black}0.17 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.32 & \color{black}0.61 & \color{black}0.07 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 &
        \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 &
        \color{black}0.61 & \color{black}0.32 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17
    \end{pmatrix}}_{=\bm{Z}^\ast_{k}\in\R^{n^\ast\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.2cm}
  {\transparent{0}\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.38cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\transparent{0}\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Initializing a base learner with binning}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 234 \\ 73 \\ 498 \\ \vdots \\ 112 \\ 261 \\ 343 \end{pmatrix}}_{= \bm{x}_{k}\in\R^n}$}
    {\transparent{0.2}\normalsize $\Rightarrow$}
    {\transparent{0.2}\tiny $\underbrace{
      \begin{pmatrix}
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 & \color{black}0.62 & \color{black}0.31 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{black}0.20 & \color{black}0.66 & \color{black}0.14 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17\color{black}\\
        \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots &
          \color{black}\vdots & \color{black}\vdots & \color{black}\vdots\\
        \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 & \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.01 & \color{black}0.40 & \color{black}0.55 & \color{black}0.04 &
          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.29 & \color{black}0.63 &
          \color{black}0.08 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
      \end{pmatrix}
    }_{=\bm{Z}_{k}\in\R^{n\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.1cm}
  {\transparent{0.2}\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.5cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\transparent{0.2}\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
  \phantom{a.}\includegraphics[width=9cm]{figures/binx-iter2.png}
  \begin{minipage}{0.85\textwidth}
    {\tiny $\underbrace{\begin{pmatrix} 7.0 \\ 129.8 \\ 252.5 \\ 375.2 \\ 498.0 \end{pmatrix}}_{= \bm{z}_{k}\in\R^{n^\ast}}$}
  {\normalsize $\Rightarrow$}
  {\tiny $\underbrace{
    \begin{pmatrix}
      \color{black}0.17 & \color{black}0.67 & \color{black}0.17 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.32 & \color{black}0.61 & \color{black}0.07 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 &
        \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 &
        \color{black}0.61 & \color{black}0.32 & \color{lightgray}0.00 & \color{lightgray}0.00\\
      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
        \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17
    \end{pmatrix}}_{=\bm{Z}^\ast_{k}\in\R^{n^\ast\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.2cm}
  {\normalsize $\Rightarrow$}
  \end{minipage}\hspace{-0.38cm}
  \begin{minipage}{0.13\textwidth}
    \vspace{-0.3cm}
    \parbox{0.5\linewidth}{\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
  \end{minipage}
  \addtocounter{framenumber}{-1}
\end{frame}


%\begin{frame}{Binning a base learner}
%  \begin{itemize}
%    \item
%      Each base learner \(b_1, \dots, b_K\) requires to build a design
%      matrix \(\bm{Z}_k\in\mathbb{R}^{n\times d_k}\) based on the feature
%      vector \(\bm{x}_k\).
%    \item
%      For example:
%  \end{itemize}
%
%  \begin{minipage}{0.85\textwidth}
%    {\tiny $\underbrace{\begin{pmatrix} 234 \\ 73 \\ 498 \\ \vdots \\ 112 \\ 261 \\ 343 \end{pmatrix}}_{= \bm{x}_{k}\in\R^n}$}
%    {\normalsize $\Rightarrow$}
%    {\tiny $\underbrace{
%      \begin{pmatrix}
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 & \color{black}0.62 & \color{black}0.31 &
%          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
%        \color{lightgray}0.00 & \color{black}0.20 & \color{black}0.66 & \color{black}0.14 & \color{lightgray}0.00 & \color{lightgray}0.00 &
%          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
%          \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17\color{black}\\
%        \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots & \color{black}\vdots &
%          \color{black}\vdots & \color{black}\vdots & \color{black}\vdots\\
%        \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 & \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 &
%          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.01 & \color{black}0.40 & \color{black}0.55 & \color{black}0.04 &
%          \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.29 & \color{black}0.63 &
%          \color{black}0.08 & \color{lightgray}0.00 & \color{lightgray}0.00\color{black}\\
%      \end{pmatrix}
%    }_{=\bm{Z}_{k}\in\R^{n\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.1cm}
%  {\normalsize $\Rightarrow$}
%  \end{minipage}\hspace{-0.5cm}
%  \begin{minipage}{0.13\textwidth}
%    \vspace{-0.3cm}
%    \parbox{0.5\linewidth}{\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
%  \end{minipage}
%  \begin{itemize}
%    \item[$\Rightarrow$] If $n$ is large, the memory gets filled very fast (especially if $p$ is also large).
%  \end{itemize}
%\end{frame}



%\begin{frame}{Binning a base learner}
%  \begin{itemize}
%    \item
%      Represent numerical features \(\bm{x}_k\) by \(n^\ast\) design points
%      \(\bm{z}_k\).
%    \item
%      Build the design matrix \(\bm{Z}_k^\ast\) based on \(\bm{z}_k\) which
%      requires to store \(n^\ast d_k\) values instead of \(nd_k\).
%    \item
%      Use optimized cross-product operations to estimate the parameters
%      \(\tbh_k^{[m]}\) of base learner \(b_k\) to also speed up the fitting.
%  \end{itemize}
%
%  \hspace{-0.2cm}
%  \begin{minipage}{0.85\textwidth}
%    {\tiny $\underbrace{\begin{pmatrix} 7.0 \\ 129.8 \\ 252.5 \\ 375.2 \\ 498.0 \end{pmatrix}}_{= \bm{z}_{k}\in\R^{n^\ast}}$}
%  {\normalsize $\Rightarrow$}
%  {\tiny $\underbrace{
%    \begin{pmatrix}
%      \color{black}0.17 & \color{black}0.67 & \color{black}0.17 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
%      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.32 & \color{black}0.61 & \color{black}0.07 & \color{lightgray}0.00 &
%        \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
%      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.02 & \color{black}0.48 & \color{black}0.48 &
%        \color{black}0.02 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00\\
%      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{black}0.07 &
%        \color{black}0.61 & \color{black}0.32 & \color{lightgray}0.00 & \color{lightgray}0.00\\
%      \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 & \color{lightgray}0.00 &
%        \color{lightgray}0.00 & \color{black}0.17 & \color{black}0.67 & \color{black}0.17
%    \end{pmatrix}}_{=\bm{Z}^\ast_{k}\in\R^{n^\ast\times d_k}\ \text{(B-spline basis)}}$}%\hspace{-0.2cm}
%  {\normalsize $\Rightarrow$}
%  \end{minipage}\hspace{-0.38cm}
%  \begin{minipage}{0.13\textwidth}
%    \vspace{-0.3cm}
%    \parbox{0.5\linewidth}{\includegraphics[width=2.5cm]{figures/binning/fig-fe.png}}
%  \end{minipage}
%\end{frame}

\newcommand{\bmult}{\ast_b}
\newcommand{\bfun}{m_M}

\begin{frame}{Binning in CWB}
    Each (univariate) base learner $b_k$ can apply binning individually to:
    \begin{itemize}
        \item 
            Reduce the memory consumption of $\design_k$.% from $nd_k$ double values to $n^\ast d_k$ double values plus an $n$ valued integer vector\footnote[frame,1]{Applies for dense matrices, the memory savings are different for saving sparse matrices}.\begin{itemize}
%                \item[$\Rightarrow$] E.g., $n = 100\phantom{.}000$, $d_k = 24$ requires $\approx 18.31$ MB without and $0.44$ MB with binning to store one $\design_k$ for $n^\ast = \sqrt{n}$.
%            \end{itemize}
        
        \item
            Use optimized matrix operations $m_M$ and $m_v$ based on binning:
            \begin{itemize}
                \item 
                    Speed up the \textbf{model initialization}: Calculation of information that do not depend on $m$ like $\design^\ast_1, \dots, \design^\ast_K$ and $\bm{F}_k^{-1} = (m_M(\design_k^{\ast}, \design_k^\ast) + \penMat_k)^{-1}$, that do not depend on $m$ prior to the fitting
                \item 
                    Speed up the \textbf{fitting process}: Estimate parameter with $\tbmh_k = \bm{F}_k^{-1} m_v(\design_k^\ast, \rmm)$ 
            \end{itemize}
      
%      \item 
%        In the \textbf{initialization}, each base learner calculates \[\xv_k \rightarrow \bm{z}_k \rightarrow \bm{Z}_k^\ast \rightarrow \bm{L}_k = \operatorname{chol}(\operatorname{binMatMat}(\design^{\ast}_k, \design^\ast_k) + \penMat_k).\]\vspace{-0.5cm}
%        \begin{itemize}
%            \item[$\Rightarrow$] Using $\operatorname{binMatMat}$ reduces the number of operations in the initialization from $K(d^2n + d^3)$ to $K(d^2n^\ast + n + d^3)$. 
%        \end{itemize}\vspace{0.2cm}
%      
%      \item 
%        During the \textbf{fitting}, the base learner estimates the parameter with \[\tbmh_k = \operatorname{cholSolve}(\bm{L}_k, \operatorname{binMatVec}(\design^{\ast\tran}_k, \rmm)).\]\vspace{-0.5cm}
%        \begin{itemize}
%            \item[$\Rightarrow$] Using $\operatorname{binMatVec}$ reduces the number of operations in each iteration from $K(d^2 + dn)$ to $K(d^2n + dn^\ast + n)$. 
%        \end{itemize}
  \end{itemize}
\end{frame}

\fSlide{Efficiency}{Benchmark result and big data examples}

%\begin{frame}{Runtime comparisons of CWB variants}
%  \begin{figure}
%    \centering
%    \includegraphics[width=\textwidth]{figures/fig-cacb-runtimes.pdf}
%  \end{figure}
%  5000 boosting iterations without early stopping.
%\end{frame}

\begin{frame}{Benchmark comparisons of CWB variants}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig-cacb-benchmark.pdf}
    \caption{\small(Figure\footnote[frame,1]{5-fold cross-validation with early stopping in each fold.} reference: \citet{schalk2022accelerated})}
  \end{figure}\vspace{-0.3cm}
  \begin{itemize}
      \item Using binning improves the runtime.
      \item Accelerating CWB can speed up the training time further without sacrificing predictive performance.
  \end{itemize}
  %{\scriptsize 5-fold cross-validation with early stopping in each fold.}
\end{frame}

\begin{frame}{Memory consumption for bigger data sets}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig-cacb-big-data.pdf}
    \caption{\small(Figure\footnote[frame,1]{Fitting was conducted for 50 iterations.} reference: \citet{schalk2022accelerated})}
  \end{figure}\vspace{-0.5cm}
  \begin{itemize}
      \item \textbf{Higgs}: 2.4 GB, $n = 11\cdot 10^6$, $p = 29$ (all numeric)
      \item \textbf{NYC Taxi}: 3.3 GB, $n = 24.3\cdot 10^6$, $p = 22$ (all numeric)
      \item \textbf{Flood Insurance}: 3.4 GB, $n = 14.5\cdot 10^6$, $p = 50$ (29 are numeric)
  \end{itemize} 
  %{\scriptsize Fitting was conducted for 50 iterations.}
\end{frame}

%\begin{frame}{Summary and Outlook}
%  \textbf{Summary:}
%  \begin{itemize}
%    \item
%      Nesterov's momentum speeds up the fitting process without suffering performance.
%    \item
%      Binning saves memory by a reduced representation and also speeds up the fitting process.
%    \item
%      A benchmark showed the effectiveness of these approaches by being significantly faster by achieving the same or better test performance.
%  \end{itemize}
%  \textbf{Outlook:}
%  \begin{itemize}
%    \item
%      Using array arithmetic to even faster calculate matrix products and the RWTP base learner.
%  \end{itemize}
%\end{frame}

\section{Distributed computing}

\newcommand{\iSite}{s}
\newcommand{\nSites}{S}
\newcommand{\doH}{\texttt{[H]}\xspace}
\newcommand{\doS}{\texttt{[S]}\xspace}
\newcommand{\algospace}{\hspace{\algorithmicindent}}
\newcommand{\lsite}{\blk_\times}

\begin{frame}{Distributed data set}
  Assume the \texttt{Country} column is not present in the data set and each country holds a its own partition:\vspace{-0.3cm}
  {\tiny
  \begin{table}
  \centering
  \begin{tabular}[t]{cccccc}
  \toprule
    \textbf{Life.expectancy} & {\color{lightgray}\textbf{Country}} & \textbf{Year} & \textbf{BMI} & \textbf{Adult.Mortality} & \textbf{Data set}\\
  \midrule
    51.2 & {\color{lightgray}ETH} & 2000 & 12.3 & 391 & \multirow{3}{*}{\color[HTML]{3B4992}\normalsize$\mathcal{D}_1$}\\
    {\tiny$\vdots$} & {\tiny\color{lightgray}$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$}\\
    64.8 & {\color{lightgray}ETH} & 2015 & 17.6 & 225\\ \hline
    78.0 & {\color{lightgray}GER} & 2000 & 55.1 & 95 & \multirow{3}{*}{\color[HTML]{EE0000}\normalsize$\mathcal{D}_2$}\\
    {\tiny$\vdots$} & {\tiny\color{lightgray}$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$}\\
    81.0 & {\color{lightgray}GER} & 2015 & 62.3 & 68\\ \hline
    79.6 & {\color{lightgray}SWE} & 2000 & 52.8 & 73 & \multirow{3}{*}{\color[HTML]{008B45}\normalsize$\mathcal{D}_3$}\\
    {\tiny$\vdots$} & {\tiny\color{lightgray}$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$}\\
    82.4 & {\color{lightgray}SWE} & 2015 & 59.5 & 53\\ \hline
    76.8 & {\color{lightgray}USA} & 2000 & 6.1  & 114 & \multirow{3}{*}{\color[HTML]{631879}\normalsize$\mathcal{D}_4$}\\
    {\tiny$\vdots$} & {\tiny\color{lightgray}$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$}\\
    79.3 & {\color{lightgray}USA} & 2015 & 69.6 & 13\\ \hline
    57.3 & {\color{lightgray}ZAF} & 2000 & 4.1 & 397 & \multirow{3}{*}{\color[HTML]{008280}\normalsize$\mathcal{D}_5$}\\
    {\tiny$\vdots$} & {\tiny\color{lightgray}$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$} & {\tiny$\vdots$}\\
    62.9 & {\color{lightgray}ZAF} & 2015 & 51.1 & 328\\
  \bottomrule
  \end{tabular}
  \end{table}}
  $\Rightarrow$ Due to privacy reasons, it is not allowed to share and merge these data sets to $\mathcal{D} = \cup_{s=1}^S \mathcal{D}_s$.
\end{frame}

\begin{frame}{Distributed data setup}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/weltkarte.png}
  \end{figure}
  \begin{itemize}
    \item Data is partitioned \textbf{horizontally}: Each of the $\nSites$ sites hold the same features but different observations.
    %\item \textbf{Vertically partitioned data}: Each site has the same observations but different features.
  \end{itemize}
%  $\Rightarrow$ Can we still fit a model with CWB?
\end{frame}

\begin{frame}{Distributed data setup}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig-sites-host.png}
  \end{figure}
  \begin{itemize}
    \item A host controls the communication with the sites, the sites cannot communicate with each other.
    \item The host is the vulnerable component.% since it can be high-jacket and access to the communicated data is out of the question.
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Distributed data setup}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig-sites-host2.png}
  \end{figure}
  \begin{itemize}
    \item The communicated data from the sites must ensure privacy of the original data sets.
  What is allowed to be shared?
  \begin{itemize}
    \item Aggregated data that does not allow reconstructing parts of the original data set.
    \item Encrypted data.% (e.g. via homomorphic encription~\citep{gentry2009fully}).
  \end{itemize}
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Distributed data setup}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/fig-sites-host2.png}
  \end{figure}
  \begin{itemize}
    \item The communicated data from the sites must ensure privacy of the original data sets.
  What is allowed to be shared?
  \begin{itemize}
    \item Aggregated data that does not allow reconstructing parts of the original data set.
    \item Encrypted data
  \end{itemize}
  \end{itemize}
  $\Rightarrow$ Is it still possible to fit a model with CWB?
  \addtocounter{framenumber}{-1}
\end{frame}

%\begin{frame}{Setup}
%  \begin{itemize}
%    \item $\nSites$ sites, each exclusively hold a data set $\mathcal{D}_\iSite$
%    \item A host in the middle controls the communication with the sites, the sites cannot communicate with each other.
%    \item The host is the vulnerable component since it can be high-jacket and access to the communicated data is out of question.
%    \item Hence, the communicated data from the sites must ensure privacy of the original data sets.
%    \item Often applies to sensitive data, e.g., most data sets with private information about individuals.
%  \end{itemize}
%  What is allowed to be shared?
%  \begin{itemize}
%    \item Aggregated data that does not allow reconstructing parts of the original data set.
%    \item Encrypted data (e.g. via homomorphic encription~\citep{gentry2009fully}).
%  \end{itemize}
%\end{frame}

\begin{frame}{Publication \citep{schalk2022distcwb}}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.55\textwidth]{figures/fig-paper-dcwb.png}}
  \end{figure}
  \vspace{-0.2cm}
  \textbf{Conbributions:}
  \begin{itemize}
    \item
      Provide a \textbf{distributed, privacy-preserving, and lossless CWB algorithm}: \[\operatorname{distCWB}(\mathcal{D}_1, \dots, \mathcal{D}_\nSites) = \operatorname{CWB}(\mathcal{D})\]
    \item
      \textbf{Allow for site-specific corrections} to account for heterogeneity in the data.
  \end{itemize}

\end{frame}

\begin{frame}{Site-specific vs. main effects}
  \begin{itemize}
    \item
      In the distributed setup, we denote $b_k$ as \textbf{shared} or \textbf{main effect} that is equal between all sites.
    \item
      Further, a main effects $b_k$ is extended by \textbf{site-specific effects} $b_{k,s}$ to allow a site-specific correction.
  \end{itemize}
\end{frame}

\begin{frame}{Site-specific vs. main effects}
  \begin{itemize}
    \item
      In the distributed setup, we denote $b_k$ as {\color{red}\textbf{shared} or \textbf{main effect}} that is equal between all sites.
    \item
      Further, a main effects $b_k$ is extended by \textbf{site-specific effects} $b_{k,s}$ to allow a site-specific correction.
  \end{itemize}
    \vspace{-0.2cm}
  \begin{align*}
  f(\xv|s) &= f_0 + \sum_{k=1}^K\left({\color{red}b_k(\xv|\tb_k)} \ {\color{lightgray}+ \sum_{s^\prime=1}^S  \mathds{1}_{\{s = s^\prime\}}b_{k,s}(\xv|\tb_{\lsite,s})}\right) \\
  &= f_0 + \sum_{k=1}^K \underbrace{{\color{red}b_k(\xv|\tb_k)}}_{\text{main effect}} \ {\color{lightgray}+ \underbrace{(b_0 \odot b_k)(\xv|\tb_{\lsite})}_{=b_{\lsite},\ \substack{\text{site-specific} \\ \text{effects}}}}
  \end{align*}\vspace{-0.4cm}
  \begin{itemize}
        \item 
            Equal to {\color{red}CWB with base learners $b_1, \dots, b_K$} 
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Site-specific vs. main effects}
  \begin{itemize}
    \item
      In the distributed setup, we denote $b_k$ as {\color{red}\textbf{shared} or \textbf{main effect}} that is equal between all sites.
    \item
      Further, a main effects $b_k$ is extended by {\color{blue}\textbf{site-specific effects} $b_{k,s}$} to allow a site-specific correction.
  \end{itemize}
    \vspace{-0.2cm}
  \begin{align*}
  f(\xv|s) &= f_0 + \sum_{k=1}^K\left({\color{red}b_k(\xv|\tb_k)} + {\color{blue} \sum_{s^\prime=1}^S  \mathds{1}_{\{s = s^\prime\}}b_{k,s}(\xv|\tb_{\lsite,s})}\right) \\
  &= f_0 + \sum_{k=1}^K \underbrace{{\color{red}b_k(\xv|\tb_k)}}_{\text{main effect}} + \underbrace{{\color{blue}(b_0 \odot b_k)(\xv|\tb_{\lsite})}}_{=b_{\lsite},\ \substack{\text{site-specific} \\ \text{effects}}}
  \end{align*}\vspace{-0.4cm}
  \begin{itemize}
        \item 
            Equal to {\color{red}CWB with base learners $b_1, \dots, b_K$} 
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Site-specific vs. main effects}
  \begin{itemize}
    \item
      In the distributed setup, we denote $b_k$ as {\color{red}\textbf{shared} or \textbf{main effect}} that is equal between all sites.
    \item
      Further, a main effects $b_k$ is extended by {\color{blue}\textbf{site-specific effects} $b_{k,s}$} to allow a site-specific correction.
  \end{itemize}
    \vspace{-0.2cm}
  \begin{align*}
  f(\xv|s) &= f_0 + \sum_{k=1}^K\left({\color{red}b_k(\xv|\tb_k)} + {\color{blue} \sum_{s^\prime=1}^S  \mathds{1}_{\{s = s^\prime\}}b_{k,s}(\xv|\tb_{\lsite,s})}\right) \\
  &= f_0 + \sum_{k=1}^K \underbrace{{\color{red}b_k(\xv|\tb_k)}}_{\text{main effect}} + \underbrace{{\color{blue}(b_0 \odot b_k)(\xv|\tb_{\lsite})}}_{=b_{\lsite},\ \substack{\text{site-specific} \\ \text{effects}}}
  \end{align*}\vspace{-0.4cm}
  \begin{itemize}
        \item 
            Equal to {\color{red}CWB with base learners $b_1, \dots, b_K$} and 
        \item 
            {\color{blue}RWTP base learners $b_{\lsite} = b_0 \odot b_k$}, $k = 1, \dots, K$, with $b_0$ modelling a latent categorical feature site $x_0\in\{1, \dots, S\}$.
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

%\begin{frame}{Site-specific vs. main effects}
%  \begin{itemize}
%    \item
%      In the distributed setup, we denote $b_k$ as \textbf{shared} or \textbf{main effect} that is equal between all sites.
%    \item
%      Further, a main effects $b_k$ is extended by \textbf{site-specific effects} $b_{k,s}$ to allow a site-specific correction.
%  \end{itemize}
%    \vspace{-0.2cm}
%  \begin{align*}
%  f(\xv|s) &= f_0 + \sum_{k=1}^K\left(b_k(\xv|\tb_k) + \sum_{s^\prime=1}^S  \mathds{1}_{\{s = s^\prime\}}b_{k,s}(\xv|\tb_{\lsite,s})\right) \\
%  &= f_0 + \sum_{k=1}^K \underbrace{b_k(\xv|\tb_k)}_{\text{main effect}} + \underbrace{(b_0 \odot b_k)(\xv|\tb_{\lsite})}_{=b_{\lsite},\ \substack{\text{site-specific} \\ \text{effects}}}
%  \end{align*}\vspace{-0.4cm}
%  \begin{itemize}
%        \item 
%            Equal to CWB with base learners $b_1, \dots, b_K$ and 
%        \item 
%            RWTP base learners $b_{\lsite} = b_0 \odot b_k$, $k = 1, \dots, K$, with $b_0$ modelling a latent categorical feature site $x_0\in\{1, \dots, S\}$.
%  \end{itemize}
%  \addtocounter{framenumber}{-1}
%\end{frame}

\fSlide{Distributed CWB}{Estimation of main effects}

\begin{frame}{Estimation of main effects}
  %We restricted CWB to fit $b_k$ using OLS by $\tbh_k = (\design_k^\tran \design_k)^{-1}\design_k^\tran \rmm$.
  \setcounter{algorithm}{1}
  \begin{algorithm}[H]
  \scriptsize
  \caption{Vanilla CWB algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{CWB}$}{$\D,\nu,M,L,b_1, \dots, b_\blK$}
      \State \tikzmk{A}Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \For{$\blk \in \{1, \dots, \blK\}$}\tikzmk{B}\boxtrans
              \State \tikzmk{A}$\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$ \quad {\color{red} $\bm{\leftarrow}$ \textbf{Distribute}}\tikzmk{B}\boxittwo{olivedrab}
              \State \tikzmk{A}$\sse_\blk = \sum_{i=1}^n(\rmi - b_\blk(\xi | \tbmh_\blk))^2$
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK\}} \sse_\blk$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_{\blk^{[m]}})$\tikzmk{B}\boxtrans
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \setcounter{algorithm}{2}
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{itemize}
    \item Each site $s$ holds a design matrix $\design_{k,\iSite}$ for the $k^{\text{th}}$ base learner and a slice of the pseudo residuals $\rmm_\iSite$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.7\textwidth]{figures/distr-lm-iter1.png}};
  %\begin{figure}
    %\centering
    %\includegraphics[width=0.6\textwidth]{figures/distr-lm-iter1.png}
  %\end{figure}
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{itemize}
    \item Each site calculates $\bm{F}_{k,\iSite} = \design_{k,\iSite}^\tran \design_{k,\iSite}$ and $\bm{u}^{[m]}_{k,\iSite} = \bm{Z}^\tran_{k,\iSite}\rmm_k$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.7\textwidth]{figures/distr-lm-iter2.png}};
  %\begin{figure}
    %\centering
    %\includegraphics[width=0.6\textwidth]{figures/distr-lm-iter2.png}
  %\end{figure}
	\addtocounter{framenumber}{-1}%
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{itemize}
    \item The sites are allowed to communicate $\bm{F}_{k,\iSite}$ and $\bm{u}_{k,\iSite}^{[m]}$ as long as \enquote{enough observations} are used. The host calculates $\bm{F}_k = \sum_{\iSite = 1}^\nSites \bm{F}_{k,\iSite}$ and $\bm{u}_k^{[m]} = \sum_{\iSite = 1}^\nSites \bm{u}_{k,\iSite}^{[m]}$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.7\textwidth]{figures/distr-lm-iter3.png}};
  %\begin{figure}
    %\centering
    %\includegraphics[width=0.6\textwidth]{figures/distr-lm-iter3.png}
  %\end{figure}
	\addtocounter{framenumber}{-1}%
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{itemize}
    \item Finally, the host can estimate $\tbh_k^{[m]} = (\bm{F}_k + \penMat_k)^{-1} \bm{u}_k$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.7\textwidth]{figures/distr-lm-iter4.png}};
	\addtocounter{framenumber}{-1}%
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{itemize}
    \item Finally, the host can estimate $\tbh_k^{[m]} = (\bm{F}_k + \penMat_k)^{-1} \bm{u}_k$. (The estimated main effects must be communicated back to sites to update the pseudo residuals $\rmm_k$.) 
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.7\textwidth]{figures/distr-lm-iter5.png}};
	\addtocounter{framenumber}{-1}%
\end{frame}

\begin{frame}{Estimation of main effects}
  \begin{algorithm}[H]
    \footnotesize
    \caption{Distributed Effect Estimation~\citep{karr2005secure}.\\
    The line prefixes \doS and \doH indicate whether the operation is conducted at the sites (\doS) or at the host (\doH).}\label{algo:lm-distr}
    \vspace{0.15cm}
    \hspace*{\algorithmicindent} \textbf{Input} Sites design matrices $\design_{k,1}, \dots, \design_{k,\nSites}$, response vectors $\rmm_1, \dots, \rmm_\nSites$ and\\
    \hspace*{\algorithmicindent} \phantom{\textbf{Input} }an optional penalty matrix $\penMat_k$.\\
    \hspace*{\algorithmicindent} \textbf{Output} Estimated parameter vector $\tbh_k$.\vspace{0.15cm}
    \hrule
    \begin{algorithmic}[1]
      \Procedure{$\operatorname{\tikzmk{A}distFit}$}{$\design_{k,1}, \dots, \design_{k,\nSites}, \rmm_1, \dots, \rmm_\nSites, \penMat_k$\tikzmk{B}\boxitthree{olivedrab}}
        \For{\tikzmk{A}$\iSite \in \{1, \dots, \nSites\}$}
          \State \doS $\bm{F}_{k,\iSite} = \design_{k,\iSite}^\tran \design_{k,\iSite}$
          \State \doS $\bm{u}_{k,\iSite} = \design_{k,\iSite}^\tran \rmm_\iSite$
          \State \doS Communicate $\bm{F}_{k,\iSite}$ and $\bm{u}_{k,\iSite}$ to the host
        \EndFor
        \State \doH $\bm{F}_k = \sum_{\iSite=1}^\nSites \bm{F}_{k,\iSite} + \penMat_k$
        \State \doH $\bm{u}_k = \sum_{\iSite=1}^\nSites \bm{u}_{k,\iSite}$\tikzmk{B}\boxtranstwo
        \State \doH \textbf{return} $\tbh_k = \bm{F}_k^{-1}\bm{u}_k$
      \EndProcedure
    \end{algorithmic}
  \end{algorithm}
  \normalsize
\end{frame}

\begin{frame}{Estimation of main effects}
  Substituting $\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$ with $\operatorname{distFit}$ in each iteration $m$ gives a first lossless distributed CWB algorithm.
  \setcounter{algorithm}{1}
  \begin{algorithm}[H]
  \scriptsize
  \caption{Vanilla CWB algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{CWB}$}{$\D,\nu,M,L,b_1, \dots, b_\blK$}
      \State \tikzmk{A}Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \For{$\blk \in \{1, \dots, \blK\}$}\tikzmk{B}\boxtrans
              \State \tikzmk{A}$\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$ \quad {\color{red} $\bm{\leftarrow}$ \textbf{Distribute}}\tikzmk{B}\boxittwo{olivedrab}
              \State \tikzmk{A}$\sse_\blk = \sum_{i=1}^n(\rmi - b_\blk(\xi | \tbmh_\blk))^2$
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK\}} \sse_\blk$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_{\blk^{[m]}})$\tikzmk{B}\boxtrans
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \setcounter{algorithm}{3}
\end{frame}

\begin{frame}{Estimation of main effects}
  Substituting $\tbmh_\blk = \left(\design_\blk^\tran \design_\blk + \bm{K}_\blk\right)^{-1} \design^\tran_\blk \rmm$ with $\operatorname{distFit}$ in each iteration $m$ gives a first lossless distributed CWB algorithm.
  \begin{algorithm}[H]
  \scriptsize
  \caption{Distributed CWB algorithm}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Site data $\D_1, \dots, \D_K$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{distCWB}$}{$\D_1, \dots, \D_K,\nu,M,L,b_1, \dots, b_\blK$}
      \State \tikzmk{A}Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \For{$\blk \in \{1, \dots, \blK\}$}\tikzmk{B}\boxtrans
              \State \tikzmk{A}$\tbmh_\blk = \operatorname{distFit(\design_{k,1}, \dots, \design_{k,\nSites}, \rmm_1, \dots, \rmm_\nSites, \penMat_k)}$\tikzmk{B}\boxittwo{olivedrab}
              \State \tikzmk{A}$\sse_\blk = \operatorname{aggregate}(\operatorname{get}(\sse_{k,1}, \dots, \sse_{k,\nSites}))$
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK\}} \sse_\blk$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_{\blk^{[m]}})$\tikzmk{B}\boxtranstwo
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \vspace{-0.6cm}
  {\small\textbf{Note:} The communication costs are a third resource that affects the efficiency.}
  \addtocounter{framenumber}{-1}
\end{frame}



\fSlide{Distributed CWB}{Estimation of site-specific effects}

\begin{frame}{Estimation of site-specific effects}
  \textbf{Example:} Without sharing sensitive data, we want to estimate site-specific effects (e.g. for \texttt{BMI}):
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fig-site-effects.png}
  \end{figure}
\end{frame}

\begin{frame}{Distributed estimation of site-specific effects}
  Site-specific effects $b_{\lsite} = b_0 \odot b_k$ are equal to a RWTP base learner with latent categorical base learner $b_0$ encoding the sites and the main effect $b_k$:
  \begin{center}
  \begin{tabular}{ccc}
    \includegraphics[align=c,width=0.3\textwidth]{figures/fig-site-effects.png} &
    {\Large $\Rightarrow$} &
    \includegraphics[align=c,width=0.4\textwidth]{figures/bs-tensor/fig-cat-num.png} \\
  \end{tabular}
  \end{center}
  \begin{align*}
    \tbmh_{\lsite}
    &= \left(\design_{\lsite}^\tran \design_{\lsite} +  \penMat_{\lsite}\right)^{-1}\design_{\lsite}^\tran \rmm
    \notag \\
    &= \left(\begin{array}{c}
         (\design_{\blk,1}^\tran \design_{\blk,1} + \lambda_0\idMat_{d_\blk} + \penMat_\blk)^{-1} \design_{\blk,1}^\tran \rmm_1 \\
         \vdots \\
         (\design_{\blk,\nSites}^\tran \design_{\blk,\nSites} + \lambda_0\idMat_{d_\blk} + \penMat_\blk)^{-1} \design_{l,\nSites}^\tran \rmm_\nSites
    \end{array}\right) =
    \left(\begin{array}{c}
      \tbmh_{{\lsite}, 1}  \\
      \vdots \\
      \tbmh_{{\lsite}, \nSites}
    \end{array}\right)\in\R^{Sd_k}
  \end{align*}
\end{frame}

\begin{frame}{Estimation of site-specific effects}
  \begin{itemize}
    \item Each site calculates and communicates the site-specific correction $\tbmh_{{\lsite},\iSite} = (\design_{\blk,\iSite}^\tran \design_{\blk,\iSite} + \lambda_0\idMat_{d_\blk} + \penMat_\blk)^{-1} \design_{\blk,\iSite}^\tran \rmm_\iSite$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.6\textwidth]{figures/fig-site-effects.png}};
\end{frame}

\begin{frame}{Estimation of site-specific effects}
  \begin{itemize}
    \item Each site calculates and communicates the site-specific correction $\tbmh_{{\lsite},\iSite} = (\design_{\blk,\iSite}^\tran \design_{\blk,\iSite} + \lambda_0\idMat_{d_\blk} + \penMat_\blk)^{-1} \design_{\blk,\iSite}^\tran \rmm_\iSite$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.6\textwidth]{figures/fig-site-effects-iter1.png}};
	\addtocounter{framenumber}{-1}%
\end{frame}

\begin{frame}{Estimation of site-specific effects}
  \begin{itemize}
    \item The host collects all site parameters $\tbmh_{{\lsite},1}, \dots, \tbmh_{{\lsite},\nSites}$ to reconstruct the parameter vector $\tbmh_{\lsite}$ of the RWTP base learner $b_{\lsite}$.
  \end{itemize}
  \tikz[remember picture, overlay] \node[anchor=center] at ($(current page.center)-(0,1.1)$) {\includegraphics[width=0.6\textwidth]{figures/fig-site-effects-iter2.png}};
	\addtocounter{framenumber}{-1}%
\end{frame}


\begin{frame}{Estimation of site-specific effects}
  \vspace{-0.2cm}
  \begin{algorithm}[H]
  \scriptsize
  \caption{Distributed CWB algorithm}\label{algo:dcwb}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Site data $\D_1, \dots, \D_K$, learning rate $\nu$, number of boosting iterations $M$, loss\\
  \hspace*{\algorithmicindent} \phantom{\textbf{Input} } function $L$, base learners $b_1, \dots, b_\blK$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{distCWB}$}{$\D_1, \dots, \D_K,\nu,M,L,b_1, \dots, b_\blK$}
      \State \tikzmk{A}Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \For{$\blk \in \{1, \dots, \blK\}$}\tikzmk{B}\boxtrans
              \State \tikzmk{A}$\tbmh_\blk = \operatorname{distFit(\design_{k,1}, \dots, \design_{k,\nSites}, \rmm_1, \dots, \rmm_\nSites, \penMat_k)}$
              \State$\tbmh_{\lsite} = \operatorname{getSiteEffects}(\tbmh_{{\lsite}, 1}, \dots, \tbmh_{{\lsite},\nSites})$\tikzmk{B}\boxittwo{olivedrab}
              \State \tikzmk{A}$\sse_\blk = \operatorname{aggregate}(\operatorname{get}(\sse_{k,1}, \dots, \sse_{k,\nSites}))$
              \State $\sse_{\lsite} = \operatorname{aggregate}(\operatorname{get}(\sse_{\lsite,1}, \dots, \sse_{\lsite,\nSites}))$\tikzmk{B}\boxtrans
          \EndFor
          \State $\blk^{[m]} = \argmin_{\blk\in\{1, \dots, \blK, 1_{\times}, \dots, \blK_{\times}\}} \sse_\blk$\hspace{0.4cm}
          \State \tikzmk{A}$\fmh(\xv) = \fmdh(\xv) + \nu b_{\blk^{[m]}} (\xv | \tbmh_{\blk^{[m]}})$\tikzmk{B}\boxtrans
      \EndWhile
      \State \textbf{return} $\fh = \fmh$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \vspace{-0.5cm}
  {\small\textbf{Note:} The algorithm requires further refinements like sharing of main effects from host to sites or sharing of the SSE values from the sites to the host.}
\end{frame}

\begin{frame}{Distributed CWB algorithm}
   \begin{figure}
       \centering\vspace{-0.2cm}
       \includegraphics[width=0.7\textwidth]{figures/fig-dcwb.png}
   \end{figure} 
\end{frame}

\fSlide{Distributed CWB}{Comparison with pooled approaches}

%\begin{frame}{Additional comments}
%  \begin{itemize}
%    \item
%      The distributed CWB algorithm (Algorithm~\ref{algo:dcwb}) must also consider
%      \begin{itemize}
%          \item to communicate $\tbmh_{k^{[m]}}$ from the host to the sites so that they can calculate $\rmm_s$ locally and
%          \item the sites must share the SSE values of the main and site-specific effects to enable the base learner selection.
%      \end{itemize}
%    \item
%      Besides runtime or memory, communication costs are a third component that affects efficiency. 
%    \item
%      Algorithm~\ref{algo:dcwb} requires communicating $2Kd + 2K$ values in each iteration from each site to the host.
%    \item
%      An additional penalty $\lambda_0$ is added to the first-order differences of the site-specific effects and allows to favor the main effect over the site-specific effects.
%    \item
%      Algorithm~\ref{algo:dcwb} is implemented in DataSHIELD~\citep{gaye2014datashield} and available on GitHub \footnote[frame,1]{\url{https://github.com/schalkdaniel/dsCWB}}.
%  \end{itemize}
%\end{frame}


\begin{frame}{Comparison with pooled approaches}
  \begin{itemize}
    \item \textbf{Reminder:} The proposed algorithm is a lossless and distributed pendant to CWB on the merged data.
    \item Instead of benchmarking the algorithm, we compared it with a GAMM fitted with \texttt{mgcv}.
  \end{itemize}
%  \begin{figure}
%    \centering
%    \includegraphics[width=0.9\textwidth]{figures/fig-dcwb-effect-decomposition.pdf}
%  \end{figure}
%  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig-dcwb-effect-comparison.pdf}
    \caption{\small(Figure reference: \citet{schalk2022distcwb})}
  \end{figure}
\end{frame}


% \begin{frame}{Summary and outlook}
%   \textbf{Summary:}
%   \begin{itemize}
%     \item
%       CWB can be fit to distributed data in a lossless fashion by just relying on aggregated data that do not reveal private information about the used data set.
%     \item
%       Main effects are estimated by calculating a distributed linear model in each iteration.
%     \item
%       Site-specific effects can be estimated by making use of the structure of a RWTP base learner.
% %    \item
% %      The SSE calculation is also done easily ba sharing the SSE per site and aggregating them.
%   \end{itemize}
%   \textbf{Outlook:}
%   \begin{itemize}
%     \item Account for vertically and horizontally distributed data.
%     \item Reduce the communication costs to speed up the fitting process.
%   \end{itemize}
% \end{frame}



%%%%% SHORT SUMMARIES, MAX 2 SLIDES ---------------------------------------------------------------%%


\section{Further contributions}

\begin{frame}{Publication \citep{schalk2018compboost}}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.6\textwidth]{figures/fig-compboost-paper.png}}
  \end{figure}
  \vspace{-0.2cm}
  \textbf{Contributions:}
  \begin{itemize}
    \item
      Efficient and object-oriented CWB implementation.
  \end{itemize}
\end{frame}


\begin{frame}{About}
  \begin{minipage}[t]{0.18\textwidth}
    \includegraphics[width=\linewidth]{figures/fig-compboost-logo.png}
  \end{minipage}
  \begin{minipage}{0.8\textwidth}
  \begin{itemize}
    \item
      \texttt{compboost} is an \texttt{R} package that implements CWB.
    \item
      The core is implemented in \texttt{C++} for faster runtime and exported via \texttt{Rcpp} to \texttt{R}.
    \item
      \texttt{mlr3} learners to, e.g., evaluate and tune the model.
    \item
      Parallelized model fitting with \texttt{OpenMP} and model export as \texttt{JSON}.
  \end{itemize}
  \end{minipage}
\end{frame}

\begin{frame}[fragile]{Demo}
  \vphantom{code}
  {
  \scriptsize
  \begin{Shaded}
\begin{verbatim}
library(compboost)

cb = boostComponents(spam, "type", iterations = 0, df = 5)
cb$addTensor("money", "your")
cb$train(1000)

plotBaselearnerTraces(cb) | plotPEUni(cb, "charExclamation") |
  plotTensor(cb, "money_your_tensor") | plotFeatureImportance(cb, 10)
\end{verbatim}
  \end{Shaded}
  }
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/fig-cwb-demo.png}
  \end{figure}
\end{frame}

\begin{frame}{Speedup compared to mboost}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig-cacb-mboost-comparison.pdf}
    \caption{\small(Figure reference: \citet{schalk2022accelerated})}
  \end{figure}
  \vspace{-0.3cm}
  \begin{itemize}
    \item The pure implementation (CWB, green) is up to 5 times and with binning up to 10 times faster than \texttt{mboost}. 
    \item ACWB and HCWB elevate the speedup even more.
  \end{itemize}
\end{frame}


%\begin{frame}{Summary and Outlook}
%  \textbf{Summary:}
%  \begin{itemize}
%    \item Fast and flexible toolbox for CWB.
%    \item Allows defining custom losses and base learner.
%    \item High-level functionality to analyze the model.
%  \end{itemize}
%  \textbf{Outlook:}
%  \begin{itemize}
%    \item Better binning support for RWTP base learner.
%    \item Support for more complex tasks, e.g., location, scale, and shape.
%  \end{itemize}
%\end{frame}

\begin{frame}{Publication \citep{coors2021autocompboost}}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.7\textwidth]{figures/fig-paper-autocwb.png}}
  \end{figure}
%  \vspace{-0.4cm}
  \textbf{Contributions:}
  \begin{itemize}
    \item
      Interpretable automated ML (AutoML) system based on three stages with increasing complexity and CWB as the fitting engine.
    \item
      Tools to assess the required model complexity and the decision-making process.
  \end{itemize}
\end{frame}


%\begin{frame}{About}
%  \begin{itemize}
%    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
%      \begin{itemize}
%        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
%        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
%        \item Deeper interactions $f_{\text{deep}}$ with trees
%      \end{itemize}
%  \end{itemize}
%\end{frame}
%
%\begin{frame}{About}
%  \begin{itemize}
%    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
%      \begin{itemize}
%        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
%        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
%        \item Deeper interactions $f_{\text{deep}}$ with trees
%      \end{itemize}
%    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.8\textwidth]{figures/fig-acwb-ml-pipeline.png}
%      \end{figure}
%  \end{itemize}
%	\addtocounter{framenumber}{-1}
%\end{frame}
%
%\begin{frame}{About}
%  \begin{itemize}
%    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
%      \begin{itemize}
%        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
%        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
%        \item Deeper interactions $f_{\text{deep}}$ with trees
%      \end{itemize}
%    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-ml-pipeline.png}
%      \end{figure}
%    \item Using boosting allows to gain a fine grid of the risk improvement for each stage:
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.7\textwidth]{figures/fig-acwb-risk.png}
%      \end{figure}
%  \end{itemize}
%	\addtocounter{framenumber}{-1}
%\end{frame}
%
%\begin{frame}{About}
%  \begin{itemize}
%    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
%      \begin{itemize}
%        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
%        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
%        \item Deeper interactions $f_{\text{deep}}$ with trees
%      \end{itemize}
%    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-ml-pipeline.png}
%      \end{figure}
%    \item Using boosting allows to gain a fine grid of the risk improvement for each stage:
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-risk.png}
%      \end{figure}
%    \item Explaining the framework is done by using the CWBs interpretation functionality.
%  \end{itemize}
%	\addtocounter{framenumber}{-1}
%\end{frame}
%
%
%
%
%\begin{frame}{Limitation and Outlook}
%  \textbf{Limitations:}
%  \begin{itemize}
%    \item Detecting interactions is done heuristically by a surrogate random forest and hence on a different model class than stage one and two.
%    \item It is not clear how switching the model class in stage can revert the effects of stage one and two.
%
%  \end{itemize}
%  \textbf{Outlook:}
%  \begin{itemize}
%    \item Other techniques to detect interactions and simulations to show their effectiveness.
%    \item Focus on the third stage. An idea is to orthogonalize $f_{\text{deep}}$ by $f_{\text{uni}} + f_{\text{pint}}$ to ensure that their effects remain untouched.
%  \end{itemize}
%\end{frame}

%\section{Distributed model evaluation}

\begin{frame}{Publications \citep{schalk2022dauc,schalk2022dsBinVal}}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.472\textwidth]{figures/fig-paper-dauc.png}}
    \frame{\includegraphics[width=0.42\textwidth]{figures/fig-paper-dsBinVal.png}}
  \end{figure}
  %\vspace{-0.4cm}
  \textbf{Contributions:}
  \begin{itemize}
    \item
      Privacy-preserving and distributed evaluation based on the AUC.
    \item
      Implementation in \texttt{DataSHIELD}~\citep{gaye2014datashield} to validate binary classification models.
  \end{itemize}
\end{frame}

%\begin{frame}{Challenge}
%  Many performance measures $\rho(\yv, \hat{\yv})$ that are based on a point-wise loss $L_\rho(y, \fh(\xv))$ can be calculated securely by:
%  \begin{itemize}
%    \item Sharing $l_\iSite = \sum_{(\xv, y)\in\D_\iSite} L_\rho(y,\fh(\xv))$
%    \item Calculating $\rho(\yv,\hat{\yv}) = \sum_{\iSite=1}^\nSites w_\iSite l_\iSite$ (e.g. $L_\rho(y,\fh(\xv)) = (y - \fh(\xv)^2)$ and $w_\iSite = 1$ for $\rho = \operatorname{SSE}$)
%  \end{itemize}
%\textbf{But:}
%  \begin{itemize}
%    \item The AUC requires global information about the predictions scores (the order) for calculation.
%    \item Merging these objects is not allowed without security concerns.
%  \end{itemize}
%\end{frame}
%
%
%\begin{frame}{About}
%  \begin{itemize}
%    \item Initialized by \texttt{DIFUTURE}~\citep{DIFUTURE2018} to validate a treatment decision score for multiple sclerosis patients.
%    \item Privacy-preserving and distributed calculation of the ROC-GLM as parametric approximation of the empirical ROC curve.
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.4\textwidth]{figures/fig-dauc-appr.png}
%      \end{figure}
%  \end{itemize}
%\end{frame}
%
%\begin{frame}{About}
%  \begin{itemize}
%    \item Initialized by \texttt{DIFUTURE}~\citep{DIFUTURE2018} to validate a treatment decision score for multiple sclerosis patients.
%    \item Privacy-preserving and distributed calculation of the ROC-GLM as parametric approximation of the empirical ROC curve.
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.1\textwidth]{figures/fig-dauc-appr.png}
%      \end{figure}
%    \item Privacy is ensured by relying on aggregations as well as incorporating differential privacy~\citep{dwork2006differential}.
%      \begin{figure}
%        \centering
%        \includegraphics[width=0.7\textwidth]{figures/fig-dauc-algo.png}
%      \end{figure}
%  \end{itemize}
%	\addtocounter{framenumber}{-1}
%\end{frame}

%\section{\texttt{compboost}}

\section{Conclusion and outlook}

\begin{frame}{Conclusion}
      The presented adaptions to CWB improve the algorithm in several aspects:
      \begin{itemize}
        \item
          \textbf{Efficiency}~\citep{schalk2022accelerated}
        \begin{itemize}
          \item[$\Rightarrow$]
            CWB for big data (Software: \texttt{compboost}~\citep{schalk2018compboost}).
        \end{itemize}
        \item
          \textbf{Distributed computing}~\citep{schalk2022distcwb}
            \begin{itemize}
              \item[$\Rightarrow$]
                Fit CWB to (horizontally) distributed data sets by preserving privacy (Software: \texttt{dsCWB}).
            \end{itemize}
        \item \textbf{Automation}~\citep{coors2021autocompboost}
          \begin{itemize}\item[$\Rightarrow$]
            Easy access to CWB also for non-experts by a multi-stage approach (Software: \texttt{Autocompboost}).
          \end{itemize}
      \end{itemize}
%      Additionally: Distributed and privacy-preserving ROC analysis~\citep{schalk2022dauc} (Software: \texttt{dsBinVal}~\citep{schalk2022dsBinVal}).
\end{frame}

\begin{frame}{Outlook}
    \textbf{Efficiency:}
    \begin{itemize}
        \item 
            \texttt{compboost}: Better binning support and array arithmetic to accelerate the fitting for RWTP base learners.
    \end{itemize}
    \textbf{Distributed computing:}
    \begin{itemize}
        \item 
            $\operatorname{distCWB}$: Account for vertically and horizontally distributed data and reduce communication costs.
        \item
            A general framework for distributed model evaluation. 
    \end{itemize}
    \textbf{Automation:}
    \begin{itemize}
        \item 
            Focus on the third stage: 
            \begin{itemize}
                \item Investigate how problematic the switch in the model class is.
                \item This relates to detecting the relevant interactions (based on a random forest) and the base learner (tress) used in the third stage. 
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Outlook}
  \begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/topics-outlook.png}
  \end{figure}
  $\Rightarrow$ Framework that combines all the presented aspects:
    \begin{itemize}
      \item Distributed \texttt{Autocompboost} that fits a privacy-preserving CWB variant to horizontally and vertically distributed data sets.\vspace{0.1cm}
      \item Provide practitioners with insights about the required complexity, feature importance, main effects and site-specific corrections, as well as transparent decision-making.
    \end{itemize}
\end{frame}

\setbeamercolor{background canvas}{bg=metropolis_theme_color}
\begin{frame}[plain]{}
    \centering\vspace{4cm}
    {\LARGE\bfseries \color{white}Thank you for your attention!}
    \addtocounter{framenumber}{-1}
\end{frame}
\setbeamercolor{background canvas}{bg=white}

%%%%% BACKUP SLIDES ------------------------------------------------------------------------------%%

\backupbegin
\fSlide{\phantom{Backup}}{Backup}

\begin{frame}[allowframebreaks]{References}
\nocite{*}
\scriptsize
\bibliography{references}

\end{frame}

\section*{Backup}

\begin{frame}{Terminology}
  \begin{itemize}

    \item
      $p$-dimensional covariate or feature vector $\xv = (x_1, \dots, x_p) \in \Xspace =  \Xspace_1 \times \cdots\times$ and target variable $y\in\Yspace$.

    \item
      Data set $\D = \Dset$ with $(\xi, \yi)$ sampled from an unknown probability distribution $\mathbb{P}_{xy}$.

    \item
      True underlying relationship $f : \Xspace^p \to \R$, $\xv \mapsto f(\xv)$.

    \item
      Goal of Machine Learning (ML) is to estimate a model $\fh = \argmin_{f} \riske(f | \D)$ with
      \begin{itemize}
        \item Empirical risk $\riske(f | \D) = n^{-1} \sum_{(\xv, y)\in\D} L(y, \fh(\xv))$ and
        \item Loss function $L : \Yspace\times\Yspace \to \R_+$, $(y,\yhat) \mapsto L(y,\yhat)$.
      \end{itemize}

    \item
      The inducer $\Ind : \mathbb{D} \times \hpspace \to \fspace$, $(\D, \hp) \mapsto \fh=\Ind_{\hp}(\D)$ gets a data set $\D\in\mathbb{D}$ with hyperparameters (HPs) $\hp\in\hpspace$.

  \end{itemize}
\end{frame}

\begin{frame}{Gradient boosting}
  \begin{itemize}
    \item
      Gradient boosting (GB) aims to estimate $f$ based on assembling weak base learners $b:\Xspace \to \Yspace, \xv \mapsto b(\xv | \tb)$ parameterized by $\tb$.

    \item
      The model estimate $\fh$ is fitted by conducting functional gradient descent $\fmdh = \fmh + \nu \hat{b}^{[m]}$ for $M$ steps. The estimated model is then $\fh = \fmh[M]$.

    \item
      To obtain the model update $\hat{b}^{[m]}$ in iteration $m$, the weak base learner $b$ is fit to pseudo residuals $\rmm$ by minimizing the SSE: $\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$

    \item
      The pseudo residuals $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh}$, $i \in \{1, \dots, n\}$, ($\rmm$ is the vector of pseudo residuals) contain the information in which direction to move $\fmh$ for a better fit to the training data $\D$.

    \item
      The fitting is initialized with $\fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$ and repeated $M$ times or until an early stopping criterion is met.
  \end{itemize}
\end{frame}

\begin{frame}{Gradient boosting -- Algorithm}
  \begin{algorithm}[H]
  \footnotesize
  \caption{GB algorithm}\label{algo:gb}
  %\vspace{0.15cm}
  \hspace*{\algorithmicindent} \textbf{Input} Train data $\D$, number of boosting iterations $M$, loss function $L$, base learner $b$\\
  \hspace*{\algorithmicindent} \textbf{Output} Model $\fh = \fmh[M]$\vspace{0.1cm}
  \hrule
  \begin{algorithmic}[1]
  \Procedure{$\operatorname{GB}$}{$\D,M, L,b$}
      \State Initialize: $f_0 = \fh^{[0]}(\xv) = \argmin_{c\in\mathcal{Y}}\riske(c|\D)$
      \While{$m \leq M$}
          \State $\rmi = -\left.\pd{\Lxyi}{f(\xi)}\right|_{f = \fmdh},\ \ \forall i \in \{1, \dots, n\}$
          \State $\tbmh = \argmin_{\tb} \sum_{i=1}^n(\rmi - b(\xi | \tb))^2$
          \State $\nu_m = \argmin_{\nu\in\R} \sum_{i=1}^n L(\rmi, \fmh + \nu \hat{b}^{[m]}(\xi | \tbmh))$
          \State $\fmh(\xv) = \fmdh(\xv) + \nu_m \hat{b}^{[m]}(\xv | \tbmh)$
      \EndWhile
      \State \textbf{return} $\fh = \fh^{[M]}$
  \EndProcedure
  \end{algorithmic}
  \end{algorithm}
  \vspace{-0.5cm}
  A common choice for the base learner in GB is, e.g., to use trees~\citep{friedman2001greedy}. Based on the base learner, further adaptions to the algorithm are made to, e.g., increase speed or predictive power~\citep{chen2015xgboost}.
\end{frame}

\begin{frame}{Basics}
  \begin{itemize}
    \item
      Compared to GB, CWB can choose from a set of $K$ base learners $b \in \{b_1, \dots, b_K\}$.

%    \item
%      The learning rate $\nu$ is fixed and not optimized by a line search.

    \item
      Often, $b_1, \dots, b_K$ are chosen to be (interpretable) statistical models and hence $f$ corresponds to a generalized additive model~\citep[GAM;][]{hastie2017generalized}: \[f(\xv) = f_0 + \sum_{k=1}^K b_k(\xv | \tb), \ \ \text{intercept}\ f_0\]

    \item
      Advantages of CWB:
      \begin{itemize}
        \item
          Feasible to get fit in high-dimensional feature spaces ($p \gg n$).

        \item
          An inherent (unbiased) feature selection.

        \item
          Interpretable/explainable partial feature effects (depending on the choice of base learners).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Base learner}
  \begin{itemize}
    \item
      From now on, each base learner $b_k$ is defined by a basis transformation $g_k : \Xspace \to \R^{d_k}$ with $g_k(\xv) = (g_{k,1}(\xv), \dots, g_{k,d_k}(\xv))^\tran$.

    \item
      The base learners are also restricted to be linear in the parameters: $b_k(\xv | \tb_k) = g_k(\xv)^\tran \tb_k$

    \item
      Due to the linearity, the sum of two base learners $b_k(\xv | \tb_l) + b_k(\xv | \tb_m)$ equals $b_k(\xv | \tb_l + \tb_m)$.

    \item
      For $n$ data points $\xi[1], \dots, \xi[n]$, each base learner defines a design matrix $\design_k = (g_k(\xi[1])^\tran, \dots, g_k(\xi[n])^\tran)^\tran\in\R^{n\times d_k}$.

    \item
      Based on the linearity and the design matrix, each base learner can be fitted by calculating the least squares estimator $\tbh_k = (\design_\blk^\tran \design_\blk) \design_k^\tran \yv$.

    \item
      Further, a base learner is allowed to include a penalization defined by a matrix $\bm{K}_k$ which extends the estimation to $\tbh_k = (\design_\blk^\tran \design_\blk + \bm{K}_k) \design_k^\tran \yv$.

  \end{itemize}
\end{frame}

\section*{ACWB}

\begin{frame}{Accelerated gradient boosting machine}
  \begin{itemize}
    \item
      Applying Nesterov's momentum in GB was already proposed by \cite{biau2019accelerated} and refined in an algorithm called Accelerated Gradient Boosting Machine (AGBM) by \cite{lu2020accelerating}:
      \begin{align*}
      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}\\
      f^{[m+1]} &= g^{[m]} + \nu {\color{red}b^{[m]}} \\
      h^{[m+1]} &= h^{[m]} + \gamma\nu / \theta_m {\color{blue}b^{[m]}_{\text{cor}}}
      \end{align*}
      \[\Rightarrow\ f^{[m+1]} = f^{[m]} + \nu b^{[m]} + \theta_m (h^{[m]} - f^{[m]})\]%\vspace{0.1cm}
    \item Momentum in the direction of $h^{[m]} - f^{[m]}$.
    \item In each iteration, two base learners ${\color{red}b^{[m]}}$ and ${\color{blue}b^{[m]}_{\text{cor}}}$.
    \item ($\theta_m = 2 / (m+2),\ m = 0, \dots, M - 1$, momentum $\gamma \in (0,1]$)
  \end{itemize}
  \addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{Accelerated gradient boosting machine}
      \begin{align*}
      g^{[m]} &= (1 - \theta_m) f^{[m]} + \theta_m h^{[m]}\\
      f^{[m+1]} &=g^{[m]} + \nu b^{[m]} \\
      h^{[m+1]} &= h^{[m]} + \nu / \theta_m \textcolor{blue}{b^{[m]}_{\text{cor}}}
      \end{align*}  
      \begin{itemize}
    \item
      A second base learner $\textcolor{blue}{b^{[m]}_{\text{cor}}}$ is fitted to \enquote{error-corrected pseudo residuals} $\bm{c}^{[m]}$:
      \[c^{[m](i)} = \rmi + \frac{m}{m+1}(c^{[m-1](i)} - \hat{b}_{\text{cor}}^{[m-1]}(\xi))\]
      This accelerates the fitting into the direction of $\bm{c}^{[m]}$.
  \end{itemize}
\end{frame}

\begin{frame}{Base learners in AGBM}
  \begin{itemize}
    \item
      \(b^{[m]}\) is fitted to pseudo residuals \(\rmm\) w.r.t.
      \(\hat{g}^{[m-1]}\) instead of \(\fmh\).

    \item
      AGBM introduces a second base learner $b^{[m]}_{\text{cor}}$ that is fitted to error-corrected pseudo residuals:
      \[c^{[m](i)} = \rmi + \frac{m}{m+1}(c^{[m-1](i)} - \hat{b}_{\text{cor}}^{[m-1]}(\xi)),\]
      with \(i = 1, \dots, n\), if \(m > 1\) and \(\bm{c}^{[m]} = \rmm\) if
      \(m = 0\).\\[0.2cm]
      $\Rightarrow$ Each iteration adds but two base learners $b^{[m]}$ and $b^{[m]}_{\text{cor}}$:
      \begin{itemize}
        \item $b^{[m]}_{\text{cor}}$ defines the momentum sequence to accelerate the fitting into the direction of the error-corrected pseudo residuals
        \item Computing a second base learner also means two double the runtime for the same number of iterations.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Simulation study to assess the estimation quality}
    To assess the estimation quality, we measured the mean integrated squared error (MISE) between a true simulated and the estimated effects from CWB, ACWB, and HCWB: 
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{figures/fig-acwb-estimation-quality.png}
        \caption{\small(Figure reference: \citet{schalk2022accelerated}) The left column shows $\operatorname{MISE}(CWB) - \operatorname{MISE}(ACWB)$ and the right column $\operatorname{MISE}(CWB) - \operatorname{MISE}(HCWB)$}
    \end{figure}
\end{frame}

\section*{Binning}

\begin{frame}{Efficiency of binning}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig-binning-mem-runtime.png}
        \caption{\small(Figure reference: \citet{schalk2022accelerated})}
    \end{figure}
\end{frame}

\begin{frame}{MISE for binning}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig-binning-mise.png}
        \caption{\small(Figure reference: \citet{schalk2022accelerated})}
    \end{figure}
\end{frame}

\begin{frame}{Binning: Grid comparison with uniformly distributed data}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/fig-bin-uni.pdf}
    \end{figure} 
\end{frame}

\begin{frame}{Binning: Grid comparison with outlier}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/fig-bin-outlier.pdf}
    \end{figure} 
\end{frame}

\begin{frame}{Binning: Grid comparison with skewed distribution ($\chi^2$)}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/fig-bin-chisq.pdf}
    \end{figure} 
\end{frame}

\begin{frame}{Binning: Grid comparison with multimodal distribution}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/fig-bin-binormal.pdf}
    \end{figure} 
\end{frame}


\section*{Efficiency: Further comparison}

\begin{frame}{Runtime comparisons of CWB variants}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/fig-cacb-runtimes.pdf}
        \caption{\small(Figure reference: \citet{schalk2022accelerated})}  
    \end{figure}
  5000 boosting iterations without early stopping.
\end{frame}

\section*{Reconstruct original data}

\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct0.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct01.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
    \end{enumerate}
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct02.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
    \end{enumerate}
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct1.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
        \item Measure the distance $m(\xv_0) = \| \design^\tran y - \design_0^\tran \hat{\yv}_0\|$.
    \end{enumerate}
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct1.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
        \item Measure the distance $m(\xv_0) = \| \design^\tran y - \design_0^\tran \hat{\yv}_0\|$.
    \end{enumerate}
    Optimize $\xv^\ast = \argmin_{\xv_0\in\R^n}(m(\xv_0))$ 
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct2.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
        \item Measure the distance $m(\xv_0) = \| \design^\tran y - \design_0^\tran \hat{\yv}_0\|$.
    \end{enumerate}
    Optimize $\xv^\ast = \argmin_{\xv_0\in\R^n}(m(\xv_0))$ 
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct3.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
        \item Measure the distance $m(\xv_0) = \| \design^\tran y - \design_0^\tran \hat{\yv}_0\|$.
    \end{enumerate}
    Optimize $\xv^\ast = \argmin_{\xv_0\in\R^n}(m(\xv_0))$ 
    \addtocounter{framenumber}{-1}
\end{frame}
\begin{frame}{Reconstruct data}
    \begin{tabular}{cc}
        \includegraphics[align=c, width=0.45\textwidth]{figures/fig-reconstruct4.png} & \begin{minipage}{0.5\textwidth}
            Given: Basis transformation $g$ and 
            \[
            \left.\begin{array}{l}
                \design^\tran \design \\
                \design^\tran y
            \end{array}\right\}\Rightarrow \tbh 
            \]%^ \\
            Also: $n = d$
        %\end{align*}
        \end{minipage}
    \end{tabular}
    \begin{enumerate}
        \item Guess new $\xv_0\in\R^n$
        \item Calculate $\design_0 = (g(\xv_0^{(1)})^\tran, \dots, g(\xv_0^{(n)})^\tran)^\tran$.
        \item Predict $\hat{\yv}_0 = \design_0 \tbh$.
        \item Measure the distance $m(\xv_0) = \| \design^\tran y - \design_0^\tran \hat{\yv}_0\|$.
    \end{enumerate}
    Optimize $\xv^\ast = \argmin_{\xv_0\in\R^n}(m(\xv_0))$ $\Rightarrow$ Calculate $y = (\design_0^\tran)^{-1}\design^\tran y$ %($\design_0$ is invertible due to $n = d$).
    \addtocounter{framenumber}{-1}
\end{frame}


\section*{Automation}

\begin{frame}{Publication [3]}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.7\textwidth]{figures/fig-paper-autocwb.png}}
  \end{figure}
  \vspace{-0.4cm}

  \textbf{Aims:}
  \begin{itemize}
    \item
      Interpretable automated ML (AutoML) system with CWB as the fitting engine.
    \item
      Assessment of the required model complexity and the decision-making process.
  \end{itemize}
\end{frame}


\begin{frame}{About}
  \begin{itemize}
    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
      \begin{itemize}
        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
        \item Deeper interactions $f_{\text{deep}}$ with trees
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{About}
  \begin{itemize}
    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
      \begin{itemize}
        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
        \item Deeper interactions $f_{\text{deep}}$ with trees
      \end{itemize}
    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
      \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/fig-acwb-ml-pipeline.png}
      \end{figure}
  \end{itemize}
	\addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{About}
  \begin{itemize}
    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
      \begin{itemize}
        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
        \item Deeper interactions $f_{\text{deep}}$ with trees
      \end{itemize}
    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
      \begin{figure}
        \centering
        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-ml-pipeline.png}
      \end{figure}
    \item Using boosting allows to gain a fine grid of the risk improvement for each stage:
      \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/fig-acwb-risk.png}
      \end{figure}
  \end{itemize}
	\addtocounter{framenumber}{-1}
\end{frame}

\begin{frame}{About}
  \begin{itemize}
    \item Build a model $f = f_{\text{uni}} + f_{\text{pint}} + f_{\text{deep}}$ based on a multi-stage approach with:
      \begin{itemize}
        \item Univariate effects $f_{\text{uni}}$ decomposed into $f_{\text{uni,linear}}$ and $f_{\text{uni,non-linear}}$
        \item Pairwise interactions $f_{\text{pint}}$ with a RWTP base learner
        \item Deeper interactions $f_{\text{deep}}$ with trees
      \end{itemize}
    \item \texttt{Autocompboost} wraps \texttt{compboost} with an AutoML pipeline:
      \begin{figure}
        \centering
        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-ml-pipeline.png}
      \end{figure}
    \item Using boosting allows to gain a fine grid of the risk improvement for each stage:
      \begin{figure}
        \centering
        \includegraphics[width=0.25\textwidth]{figures/fig-acwb-risk.png}
      \end{figure}
    \item Explaining the framework is done by using the CWBs interpretation functionality.
  \end{itemize}
	\addtocounter{framenumber}{-1}
\end{frame}




\begin{frame}{Limitation and Outlook}
  \textbf{Limitations:}
  \begin{itemize}
    \item Detecting interactions is done heuristically by a surrogate random forest and hence on a different model class than stage one and two.
    \item It is not clear how switching the model class in stage can revert the effects of stage one and two.

  \end{itemize}
  \textbf{Outlook:}
  \begin{itemize}
    \item Other techniques to detect interactions and simulations to show their effectiveness.
    \item Focus on the third stage. An idea is to orthogonalize $f_{\text{deep}}$ by $f_{\text{uni}} + f_{\text{pint}}$ to ensure that their effects remain untouched.
  \end{itemize}
\end{frame}

\section*{Distributed model evaluation}

\begin{frame}{Publications [5,6]}
  \vspace{-0.2cm}
  \begin{figure}
    \centering
    \frame{\includegraphics[width=0.472\textwidth]{figures/fig-paper-dauc.png}}
    \frame{\includegraphics[width=0.42\textwidth]{figures/fig-paper-dsBinVal.png}}
  \end{figure}
  \vspace{-0.4cm}

  \textbf{Aims:}
  \begin{itemize}
    \item
      Privacy-preserving and distributed evaluation based on the AUC.
    \item
      Implementation in \texttt{DataSHIELD}~\citep{gaye2014datashield} to validate binary classification models.
  \end{itemize}
\end{frame}

\begin{frame}{Challenge}
  Many performance measures $\rho(\yv, \hat{\yv})$ that are based on a point-wise loss $L_\rho(y, \fh(\xv))$ can be calculated securely by:
  \begin{itemize}
    \item Sharing $l_\iSite = \sum_{(\xv, y)\in\D_\iSite} L_\rho(y,\fh(\xv))$
    \item Calculating $\rho(\yv,\hat{\yv}) = \sum_{\iSite=1}^\nSites w_\iSite l_\iSite$ (e.g. $L_\rho(y,\fh(\xv)) = (y - \fh(\xv)^2)$ and $w_\iSite = 1$ for $\rho = \operatorname{SSE}$)
  \end{itemize}
\textbf{But:}
  \begin{itemize}
    \item The AUC requires global information about the predictions scores (the order) for calculation.
    \item Merging these objects is not allowed without security concerns.
  \end{itemize}
\end{frame}


\begin{frame}{About}
  \begin{itemize}
    \item Initialized by \texttt{DIFUTURE}~\citep{DIFUTURE2018} to validate a treatment decision score for multiple sclerosis patients.
    \item Privacy-preserving and distributed calculation of the ROC-GLM as parametric approximation of the empirical ROC curve.
      \begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{figures/fig-dauc-appr.png}
      \end{figure}
  \end{itemize}
\end{frame}

\begin{frame}{About}
  \begin{itemize}
    \item Initialized by \texttt{DIFUTURE}~\citep{DIFUTURE2018} to validate a treatment decision score for multiple sclerosis patients.
    \item Privacy-preserving and distributed calculation of the ROC-GLM as parametric approximation of the empirical ROC curve.
      \begin{figure}
        \centering
        \includegraphics[width=0.1\textwidth]{figures/fig-dauc-appr.png}
      \end{figure}
    \item Privacy is ensured by relying on aggregations as well as incorporating differential privacy~\citep{dwork2006differential}.
      \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/fig-dauc-algo.png}
      \end{figure}
  \end{itemize}
	\addtocounter{framenumber}{-1}
\end{frame}



\backupend
\end{document}
